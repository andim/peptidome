---
layout: post
title: Inference of maxent models
---

Infering and benchmarking of Maxent models.

{% include post-image-gallery.html filter="maxent/" %}

### Code 
#### variability_in_observables.ipynb

# How much variability is there in different observables?


```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from common import *
```


```python
L = 9
```


```python
observables = ['fi', 'fij', 'cij', 'fijk', 'cijk', 'fold_ijk']
observables_dict = {key: dict() for key in observables}
for dataset in ['train', 'test']:
    params = np.load('data/%s_observables.npz'%dataset)
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
```


```python
observables_dict['n3'] = dict()
for dataset in ['train', 'test']:
    matrix = load_matrix('data/%s_matrix_L%i.csv.gz'%(dataset, L))
    observables_dict['n3'][dataset] = calc_n3(to_aacounts(matrix))
```


```python
bins = np.linspace(-1.5, 1.5)
```


```python
plt.hist(np.log2(observables_dict['fi']['train']/observables_dict['fi']['test']).flatten(),
         bins=bins);
```


![png](notebook_files/variability_in_observables_6_0.png)



```python
plt.hist(np.log2(flatten_ij(observables_dict['fij']['train'])
                / flatten_ij(observables_dict['fij']['test'])),
         bins=bins);
plt.yscale('log')
```


![png](notebook_files/variability_in_observables_7_0.png)



```python
20**3, flatten_ij(observables_dict['fij']['test']).shape[0], 20**2 * 9 * 8, 20**2 * 8
```




    (8000, 28800, 28800, 3200)




```python
plt.hist(np.log2(observables_dict['n3']['train']/observables_dict['n3']['test']).flatten(),
         bins=bins);
plt.yscale('log')
```


![png](notebook_files/variability_in_observables_9_0.png)



```python
plt.hist(np.log2(flatten_ijk(observables_dict['fijk']['train'])
                / flatten_ijk(observables_dict['fijk']['test'])),
         bins=bins);
plt.yscale('log')
```


![png](notebook_files/variability_in_observables_10_0.png)



```python

```
#### joined_compositional_distribution.ipynb

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

import sys
sys.path.append('..')
from lib import *
from common import labels

plt.style.use('../peptidome.mplstyle')
```


```python
L = 9

datasets = ['train', 'test', 'model_ncov', 'model_nskew']
sample_matrices = {}
for dataset in datasets:
    sample_matrices[dataset] = load_matrix('data/%s_matrix_L%i.csv.gz' % (dataset, L))
```


```python
patterns_dict = {}
counts_dict = {}
for dataset in datasets:
    patterns, counts = np.unique(to_aacounts(sample_matrices[dataset]), return_counts=True, axis=0)
    patterns_dict[dataset] = patterns
    counts_dict[dataset] = counts
```


```python
bins = np.logspace(0, 3, 15)
plt.hist(counts_dict['test'], bins, histtype='step', label='test')
#plt.hist(counts_dict['model_ncov'], bins, histtype='step', label=labels['model_ncov'])
#plt.hist(counts_dict['model_nskew'], bins, histtype='step', label=labels['model_nskew'])
#plt.legend()
plt.yscale('log')
plt.xscale('log')
plt.ylabel('Number of patterns')
plt.xlabel('Pattern multiplicity')
plt.savefig('compositional_pattern_multiplicity.png')
```


![png](notebook_files/joined_compositional_distribution_3_0.png)



```python
'%e'%patterns_dict['train'].shape[0]
```




    '2.182981e+06'




```python
counts_test = []
counts_train = []
for index in np.random.randint(0, patterns_dict['train'].shape[0], 500):
    pattern = patterns_dict['train'][index]
    counts_train.append(counts_dict['train'][index])
    match = np.all(patterns_dict['test'] == pattern, axis=1)
    if np.sum(match) == 1:
        counts_test.append(int(counts_dict['test'][match]))
    else:
        counts_test.append(0)
```


```python
plt.plot(counts_train, counts_test, 'o');
```


![png](notebook_files/joined_compositional_distribution_6_0.png)



```python

```
#### entropy_analysis.ipynb

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

import sys
sys.path.append('..')
from lib import *
```


```python
L = 9
q = naminoacids
```


```python
for name, row in load_proteomes().iterrows():
    print(name, '%e'%sum(len(s) for s in fasta_iter(proteome_path(name), returnheader=False)))
```

    Human 1.151849e+07
    Mouse 1.175792e+07
    Yeast 2.936363e+06
    Cockroach 8.873761e+06
    Cattle 1.169092e+07
    Chicken 9.962143e+06
    Wheat 4.385693e+07
    Soybean 2.173783e+07
    Vaccinia 5.679500e+04
    InfluenzaB 4.721000e+03
    InfluenzaA 4.544000e+03
    CMV 6.557400e+04
    HCV 3.173000e+03
    HBV 1.792000e+03
    HSV1 4.084000e+04
    DENV 3.392000e+03
    HIV 3.568000e+03
    EBV 4.498600e+04
    Ebola 5.494000e+03
    Ecoli 1.561358e+06
    Tuberculosis 1.331651e+06
    Listeria 8.708430e+05
    Burkholderia 1.988316e+06
    Meningococcus 5.832080e+05
    StrepA 5.152790e+05
    Hpylori 4.918260e+05
    Lyme 3.816470e+05
    Tetanus 8.093520e+05
    Leprosy 5.386230e+05
    Malaria 4.173030e+06
    Chagas 9.670826e+06
    OnchocercaVolvulus 4.985618e+06



```python
df = pd.read_csv('data/entropies.csv', index_col=0)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>S</th>
      <th>E</th>
      <th>F</th>
      <th>Ehuman</th>
    </tr>
    <tr>
      <th>name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Human</th>
      <td>25.853997</td>
      <td>-2.590162</td>
      <td>-28.444159</td>
      <td>-2.590162</td>
    </tr>
    <tr>
      <th>Malaria</th>
      <td>24.456549</td>
      <td>-5.282235</td>
      <td>-29.738784</td>
      <td>-2.088056</td>
    </tr>
    <tr>
      <th>Chagas</th>
      <td>25.629811</td>
      <td>-2.977678</td>
      <td>-28.607489</td>
      <td>-2.562489</td>
    </tr>
    <tr>
      <th>Tuberculosis</th>
      <td>24.926774</td>
      <td>-4.600918</td>
      <td>-29.527692</td>
      <td>-2.779992</td>
    </tr>
    <tr>
      <th>Ecoli</th>
      <td>25.818570</td>
      <td>-2.805620</td>
      <td>-28.624189</td>
      <td>-2.277082</td>
    </tr>
  </tbody>
</table>
</div>




```python
df['length'] = [sum(len(s) for s in fasta_iter(proteome_path(name), returnheader=False)) for name in df.index]
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-7-b818f922ff32> in <module>
    ----> 1 df['length'] = [sum(len(s) for s in fasta_iter(proteome_path(name), returnheader=False)) for name in df.index if not name == 'viruses']
    

    ~/.conda/envs/py3/lib/python3.6/site-packages/pandas/core/frame.py in __setitem__(self, key, value)
       3485         else:
       3486             # set column
    -> 3487             self._set_item(key, value)
       3488 
       3489     def _setitem_slice(self, key, value):


    ~/.conda/envs/py3/lib/python3.6/site-packages/pandas/core/frame.py in _set_item(self, key, value)
       3562 
       3563         self._ensure_valid_index(value)
    -> 3564         value = self._sanitize_column(key, value)
       3565         NDFrame._set_item(self, key, value)
       3566 


    ~/.conda/envs/py3/lib/python3.6/site-packages/pandas/core/frame.py in _sanitize_column(self, key, value, broadcast)
       3747 
       3748             # turn me into an ndarray
    -> 3749             value = sanitize_index(value, self.index, copy=False)
       3750             if not isinstance(value, (np.ndarray, Index)):
       3751                 if isinstance(value, list) and len(value) > 0:


    ~/.conda/envs/py3/lib/python3.6/site-packages/pandas/core/internals/construction.py in sanitize_index(data, index, copy)
        610 
        611     if len(data) != len(index):
    --> 612         raise ValueError("Length of values does not match length of index")
        613 
        614     if isinstance(data, ABCIndexClass) and not copy:


    ValueError: Length of values does not match length of index



```python
plt.plot(df['length'], df['S'], 'o')
plt.xscale('log')
```


```python
Fhuman = df.loc['Human']["F"]
```


```python
df['DKL'] = df['Ehuman']-df['E'] + df['F'] - Fhuman
df['DKL_rand'] = np.log(q)*L-df['S']
```


```python
nats_to_bits = np.log2(np.exp(1))
```


```python
plt.hist(df['S']*nats_to_bits)
plt.axvline(np.log2(q)*L, color='k')
```




    <matplotlib.lines.Line2D at 0x7fafb2817a20>




![png](notebook_files/entropy_analysis_9_1.png)



```python
df.sort_values('S')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>S</th>
      <th>E</th>
      <th>F</th>
      <th>Ehuman</th>
      <th>DKL</th>
      <th>DKL_rand</th>
    </tr>
    <tr>
      <th>name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>HBV</th>
      <td>20.290374</td>
      <td>-5.595645</td>
      <td>-25.886019</td>
      <td>-2.412454</td>
      <td>5.741331</td>
      <td>6.671217</td>
    </tr>
    <tr>
      <th>HCV</th>
      <td>23.543850</td>
      <td>-4.021414</td>
      <td>-27.565264</td>
      <td>-2.394171</td>
      <td>2.506137</td>
      <td>3.417740</td>
    </tr>
    <tr>
      <th>HIV</th>
      <td>23.565330</td>
      <td>-3.406392</td>
      <td>-26.971722</td>
      <td>-2.101879</td>
      <td>2.776949</td>
      <td>3.396261</td>
    </tr>
    <tr>
      <th>InfluenzaA</th>
      <td>23.943078</td>
      <td>-3.242337</td>
      <td>-27.185416</td>
      <td>-1.992319</td>
      <td>2.508762</td>
      <td>3.018512</td>
    </tr>
    <tr>
      <th>DENV</th>
      <td>24.066164</td>
      <td>-3.276689</td>
      <td>-27.342853</td>
      <td>-1.953459</td>
      <td>2.424536</td>
      <td>2.895427</td>
    </tr>
    <tr>
      <th>InfluenzaB</th>
      <td>24.403003</td>
      <td>-3.243520</td>
      <td>-27.646523</td>
      <td>-2.059277</td>
      <td>1.981878</td>
      <td>2.558587</td>
    </tr>
    <tr>
      <th>Malaria</th>
      <td>24.456549</td>
      <td>-5.282235</td>
      <td>-29.738784</td>
      <td>-2.088056</td>
      <td>1.899554</td>
      <td>2.505042</td>
    </tr>
    <tr>
      <th>Ebola</th>
      <td>24.530203</td>
      <td>-3.211622</td>
      <td>-27.741825</td>
      <td>-2.166519</td>
      <td>1.747436</td>
      <td>2.431387</td>
    </tr>
    <tr>
      <th>HSV1</th>
      <td>24.893040</td>
      <td>-4.357561</td>
      <td>-29.250601</td>
      <td>-2.804701</td>
      <td>0.746418</td>
      <td>2.068550</td>
    </tr>
    <tr>
      <th>Tuberculosis</th>
      <td>24.926774</td>
      <td>-4.600918</td>
      <td>-29.527692</td>
      <td>-2.779992</td>
      <td>0.737393</td>
      <td>2.034817</td>
    </tr>
    <tr>
      <th>Lyme</th>
      <td>25.006633</td>
      <td>-4.858972</td>
      <td>-29.865605</td>
      <td>-2.398300</td>
      <td>1.039225</td>
      <td>1.954958</td>
    </tr>
    <tr>
      <th>Burkholderia</th>
      <td>25.029332</td>
      <td>-4.291638</td>
      <td>-29.320969</td>
      <td>-2.660384</td>
      <td>0.754443</td>
      <td>1.932259</td>
    </tr>
    <tr>
      <th>EBV</th>
      <td>25.091558</td>
      <td>-3.733864</td>
      <td>-28.825422</td>
      <td>-2.833468</td>
      <td>0.519133</td>
      <td>1.870033</td>
    </tr>
    <tr>
      <th>Leprosy</th>
      <td>25.228458</td>
      <td>-4.073624</td>
      <td>-29.302082</td>
      <td>-2.625925</td>
      <td>0.589776</td>
      <td>1.733132</td>
    </tr>
    <tr>
      <th>Tetanus</th>
      <td>25.304201</td>
      <td>-4.060205</td>
      <td>-29.364406</td>
      <td>-2.303780</td>
      <td>0.836177</td>
      <td>1.657389</td>
    </tr>
    <tr>
      <th>Hpylori</th>
      <td>25.432883</td>
      <td>-3.540561</td>
      <td>-28.973443</td>
      <td>-2.332328</td>
      <td>0.678948</td>
      <td>1.528708</td>
    </tr>
    <tr>
      <th>CMV</th>
      <td>25.585036</td>
      <td>-2.854813</td>
      <td>-28.439849</td>
      <td>-2.448170</td>
      <td>0.410953</td>
      <td>1.376554</td>
    </tr>
    <tr>
      <th>Listeria</th>
      <td>25.625607</td>
      <td>-3.472927</td>
      <td>-29.098534</td>
      <td>-2.302643</td>
      <td>0.515909</td>
      <td>1.335983</td>
    </tr>
    <tr>
      <th>Chagas</th>
      <td>25.629811</td>
      <td>-2.977678</td>
      <td>-28.607489</td>
      <td>-2.562489</td>
      <td>0.251859</td>
      <td>1.331780</td>
    </tr>
    <tr>
      <th>Vaccinia</th>
      <td>25.652124</td>
      <td>-3.102833</td>
      <td>-28.754957</td>
      <td>-1.920291</td>
      <td>0.871744</td>
      <td>1.309467</td>
    </tr>
    <tr>
      <th>StrepA</th>
      <td>25.652853</td>
      <td>-3.392241</td>
      <td>-29.045095</td>
      <td>-2.317461</td>
      <td>0.473844</td>
      <td>1.308737</td>
    </tr>
    <tr>
      <th>Meningococcus</th>
      <td>25.660310</td>
      <td>-3.107576</td>
      <td>-28.767886</td>
      <td>-2.360645</td>
      <td>0.423204</td>
      <td>1.301280</td>
    </tr>
    <tr>
      <th>Yeast</th>
      <td>25.817752</td>
      <td>-2.869042</td>
      <td>-28.686794</td>
      <td>-2.302663</td>
      <td>0.323743</td>
      <td>1.143839</td>
    </tr>
    <tr>
      <th>Ecoli</th>
      <td>25.818570</td>
      <td>-2.805620</td>
      <td>-28.624189</td>
      <td>-2.277082</td>
      <td>0.348507</td>
      <td>1.143021</td>
    </tr>
    <tr>
      <th>Human</th>
      <td>25.853997</td>
      <td>-2.590162</td>
      <td>-28.444159</td>
      <td>-2.590162</td>
      <td>0.000000</td>
      <td>1.107594</td>
    </tr>
    <tr>
      <th>Cattle</th>
      <td>25.876814</td>
      <td>-2.585898</td>
      <td>-28.462712</td>
      <td>-2.543528</td>
      <td>0.023816</td>
      <td>1.084776</td>
    </tr>
    <tr>
      <th>Mouse</th>
      <td>25.881926</td>
      <td>-2.542792</td>
      <td>-28.424718</td>
      <td>-2.537264</td>
      <td>0.024969</td>
      <td>1.079664</td>
    </tr>
    <tr>
      <th>Chicken</th>
      <td>25.920531</td>
      <td>-2.475510</td>
      <td>-28.396040</td>
      <td>-2.480620</td>
      <td>0.043008</td>
      <td>1.041060</td>
    </tr>
    <tr>
      <th>Soybean</th>
      <td>25.975130</td>
      <td>-2.451571</td>
      <td>-28.426700</td>
      <td>-2.309486</td>
      <td>0.159543</td>
      <td>0.986461</td>
    </tr>
    <tr>
      <th>Cockroach</th>
      <td>26.038071</td>
      <td>-2.297132</td>
      <td>-28.335203</td>
      <td>-2.281876</td>
      <td>0.124212</td>
      <td>0.923519</td>
    </tr>
    <tr>
      <th>viruses</th>
      <td>26.090938</td>
      <td>-2.182620</td>
      <td>-28.273558</td>
      <td>0.023724</td>
      <td>2.376945</td>
      <td>0.870653</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.hist(df['DKL'])
```




    (array([14.,  8.,  0.,  3.,  5.,  0.,  0.,  0.,  0.,  1.]),
     array([0.        , 0.57413309, 1.14826618, 1.72239927, 2.29653235,
            2.87066544, 3.44479853, 4.01893162, 4.59306471, 5.1671978 ,
            5.74133089]),
     <a list of 10 Patch objects>)




![png](notebook_files/entropy_analysis_11_1.png)



```python
names = ['Human', 'Mouse', 'Vaccinia', 'InfluenzaB', 'InfluenzaA', 'CMV', 'HCV', 'HSV1',
       'DENV', 'HIV', 'EBV', 'Ebola', 'Ecoli', 'Tuberculosis', 'Listeria',
       'Burkholderia', 'Meningococcus', 'StrepA', 'Hpylori',
       'Lyme', 'Tetanus', 'Leprosy', 'Malaria', 'Chagas']

colors = matplotlib.rcParams['axes.prop_cycle'].by_key()['color']
type_to_color = {'virus' : colors[0],
                 'bacterium' : colors[1],
                 'parasite' : colors[2],
                 'vertebrate' : colors[3],
                 'uniform' : colors[4]
                }
proteomes = load_proteomes()
typecolors = [type_to_color[proteomes.loc[name]['type']] for name in names]
```


```python
lim = 4.0
ys = df.loc[names]['DKL_rand']
xs = df.loc[names]['DKL']
fig, ax = plt.subplots(figsize=(6, 6))
ax.scatter(xs, ys, color=typecolors)
ax.plot([0, lim], [0, lim], 'k-')
ax.set_xlabel('$D_{KL}$(proteome $\parallel$ human)')
ax.set_ylabel('$D_{KL}$(proteome $\parallel$ uniform)')
ax.set_aspect('equal')
ax.set_xlim(-0.0, lim)
ax.set_ylim(-0.0, lim)
for i, (type_, color) in enumerate(type_to_color.items()):
    ax.text(0.8, 0.2-i*0.03, type_, color=color, transform=ax.transAxes)
fig.tight_layout()
fig.savefig('DKLs.png')
```


![png](notebook_files/entropy_analysis_13_0.png)



```python

```
#### translation_invariance.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from common import *
```


```python
L = 9
```


```python
observables = ['fi', 'fij', 'cij', 'fijk', 'cijk', 'fold_ijk']
observables_dict = {key: dict() for key in observables}
for dataset in ['train', 'test']:
    params = np.load('data/%s_observables_L%i.npz'%(dataset, L))
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
```


```python
fi = observables_dict['fi']['train']
plt.hist((np.std(fi, axis=0)/np.abs(np.mean(fi, axis=0))).flatten(), bins=np.linspace(0.0, 0.03, 50));
```


![png](notebook_files/translation_invariance_3_0.png)



```python
plt.plot(np.std(fi, axis=0)/np.mean(fi, axis=0), 'o')
plt.ylabel('coefficient of variation')
plt.xticks(range(len(aminoacids)), (list(aminoacids)));
```


![png](notebook_files/translation_invariance_4_0.png)


--> Weak dependence on position for Methionine (edge effect because this is the amino acid coded for by the start codon), all others essentially zero


```python
fijs = np.array([list(observables_dict['fij']['train'][i, i+1]) for i in range(1, L-1)])
plt.hist((np.std(fijs, axis=0)/np.abs(np.mean(fijs, axis=0))).flatten());
```


![png](notebook_files/translation_invariance_6_0.png)


--> pair frequencies between nearest neighbors are (nearly) translation invariant


```python
fijs = observables_dict['fij']['train'][0, 1:]
plt.hist((np.std(fijs, axis=0)/np.abs(np.mean(fijs, axis=0))).flatten());
```


![png](notebook_files/translation_invariance_8_0.png)


--> pair frequencies depend on distance


```python
plt.imshow(np.std(fijs, axis=0)/np.abs(np.mean(fijs, axis=0)))
plt.colorbar(label='coefficient of variation of fij')
set_aminoacidslabel(plt.gca(), aminoacids)
```


![png](notebook_files/translation_invariance_10_0.png)



```python
fijks = np.array([list(observables_dict['fijk']['train'][i, i+1, i+2]) for i in range(1, L-2)])
plt.hist((np.std(fijks, axis=0)/np.abs(np.mean(fijks, axis=0))).flatten());
```


![png](notebook_files/translation_invariance_11_0.png)



```python
fijks = np.array([list(observables_dict['fijk']['train'][0, i, j]) for i in range(1, L) for j in range(i+1, L)])
plt.hist((np.std(fijks, axis=0)/np.abs(np.mean(fijks, axis=0))).flatten());
```


![png](notebook_files/translation_invariance_12_0.png)



```python

```
#### densityofstates.ipynb

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

import sys
sys.path.append('..')
from lib import *
from common import labels, model_hierarchy

plt.style.use('../peptidome.mplstyle')
```


```python
L = 9

```


```python
datasets = ['train', 'test', 'independent']
datasets.extend(model_hierarchy)
sample_matrices = {}
for dataset in datasets:
    sample_matrices[dataset] = load_matrix('data/%s_matrix_L%i.csv.gz' % (dataset, L))
```


```python
params = np.load('data/Human_model_9.npz')
hi = params['hi']
Jij = params['Jij']

@njit
def energy(x):
    return energy_potts(x, hi, Jij)
```


```python
params = np.load('data/Human_nskew_9.npz')
h = params['h']
J = params['J']
J2 = params['J2']
@njit
def energy(x):
    return energy_nskew(x, h=h, J=J, J2=J2)
```


```python
params = np.load('data/Human_nskewfcov_9.npz')
h = params['h']
J = params['J']
J2 = params['J2']
hi = params['hi']
Jij = params['Jij']
@njit
def energy(x):
    return energy_nskewfcov(x, h=h, J=J, J2=J2, hi=hi, Jij=Jij)
```


    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    <ipython-input-11-c5bc08616eda> in <module>
          3 J = params['J']
          4 J2 = params['J2']
    ----> 5 hi = params['hi']
          6 Jij = params['Jij']
          7 @njit


    ~/.conda/envs/py3/lib/python3.6/site-packages/numpy/lib/npyio.py in __getitem__(self, key)
        264                 return self.zip.read(key)
        265         else:
    --> 266             raise KeyError("%s is not a file in the archive" % key)
        267 
        268 


    KeyError: 'hi is not a file in the archive'



```python
energies = {}
datasets = ['test', 'independent']
datasets.extend(model_hierarchy)
for dataset in datasets:
    energies[dataset] = np.array([energy(x) for x in sample_matrices[dataset]])
```


```python
len(energies['test']), len(energies['model']), len(energies['model_ncov'])
```




    (5367489, 5421764, 5421764)




```python
bins = np.linspace(-6, 16, 100)
fig, axes = plt.subplots(figsize=(4.5, 1.8), ncols=2)
#for dataset in ['test', 'independent', 'model', 'cov', 'third']:
datasets = ['test', 'independent']
datasets.extend(model_hierarchy)
xmax = 16
xmin = -6
nbins = 200
for ax in axes:
    plot_histograms([-energies[dataset] for dataset in datasets],
                    [labels[dataset] for dataset in datasets],
                    step=True, nbins=nbins, xmin=xmin, xmax=xmax, lw=0.5, ax=ax, scaley=nbins/(xmax-xmin))
    ax.set_xlabel('-Energy')
    ax.set_ylabel('Density')
axes[0].set_ylim(0.0)
#for dataset in datasets:
#    ax.hist(-energies[dataset], bins=bins, histtype='step', label=labels[dataset], density=True)
axes[0].legend(fontsize='xx-small', ncol=1, loc='upper right')
axes[1].get_legend().remove()
axes[1].set_yscale('log')
fig.tight_layout()
fig.savefig('density_of_states.png')
fig.savefig('../../paper/images/dos.pdf')
```


![png](notebook_files/densityofstates_8_0.png)



```python
J2 = np.load('data/Human_nskew_9.npz')['J2']
J2diag = [J2[alpha, alpha, alpha] for alpha in range(naminoacids)]
bins = np.linspace(np.amin(J2), np.amax(J2))
plt.hist(J2.flatten(), density=True, histtype='step', bins=bins)
plt.hist(J2diag, density=True, histtype='step', bins=bins);
```


![png](notebook_files/densityofstates_9_0.png)



```python
list(zip(np.load('data/Human_nskewdiag_9.npz')['J2'], J2diag))
```




    [(0.005528579350366695, 0.006989415527834285),
     (-0.000490488510589386, -0.007566881525718562),
     (0.004746111563919675, 0.00960861251187901),
     (0.003457890011867693, 0.005908306975252388),
     (0.00011441873709671727, -0.0021344630454764226),
     (0.006017145887626679, 0.006978661140310664),
     (0.013773023874094438, 0.01936753975114878),
     (-0.001436246058026596, -0.0022720288923674136),
     (0.0008751561144371233, 0.002550564223574996),
     (0.002105243600159295, 0.0034507075171602785),
     (0.0037598803093053575, 0.0010071808361185397),
     (0.00045871615905420196, -0.0007060915746779927),
     (0.0018890560215912796, 0.001530306691775807),
     (0.009619951255021778, 0.0121945972209229),
     (0.0007462292826030457, 0.0015228379066869859),
     (0.0035611366550557582, 0.005001476133051156),
     (0.00749708308767569, 0.01265655979448924),
     (-0.00010223717932330546, -0.0008469126802771246),
     (0.008270760354473253, 0.004324884805089397),
     (0.0005611679924984609, -0.0026048652136603737)]




```python
[''.join(row) for row in map_numbertoaa(sample_matrices['test'][energies['test'] < -10])]
```




    ['GAAAAAAGA',
     'AAAAAAGAP',
     'GPGPPPPEP',
     'LRLLALLLL',
     'RLLALLLLL',
     'LLALLLLLL',
     'LALLLLLLA',
     'PLAPPPQPP',
     'PPPQPPASP',
     'PPPPGPTAP',
     'SPSLSSSSS',
     'PSLSSSSSS',
     'SLSSSSSSS',
     'LSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSN',
     'SSSSSSSNA',
     'GCSSSSSSS',
     'SSSSSSSAS',
     'SSSSSSASL',
     'GRPGGGRGG',
     'PPGRPGGGG',
     'TPAPGPPPP',
     'PAPGPPPPP',
     'APGPPPPPP',
     'PGPPPPPPP',
     'GPPPPPPPA',
     'PPPPPPPAP',
     'PPPPPPAPP',
     'PPPPPAPPQ',
     'PPPPAPPQQ',
     'PPQQQPPPP',
     'PQQQPPPPP',
     'QQQPPPPPP',
     'QQPPPPPPP',
     'QPPPPPPPA',
     'PPPPPPPAP',
     'PPPPPPAPP',
     'PPPPPAPPP',
     'PPPPAPPPG',
     'PPPAPPPGP',
     'PPAPPPGPG',
     'PAPPPGPGP',
     'APPPGPGPA',
     'PPPGPGPAP',
     'PPGPGPAPP',
     'PPPPPPPQV',
     'PPPGPAPAA',
     'PPGPAPAAA',
     'PGPPPPASP',
     'GPPPPASPP',
     'PPPPASPPG',
     'AAGAGGGGG',
     'AGAGGGGGG',
     'GAGGGGGGG',
     'AGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGA',
     'GGGGGGGAG',
     'GGGGGGAGG',
     'GGGGGAGGG',
     'GGGGAGGGG',
     'GGGAGGGGG',
     'GGAGGGGGG',
     'HAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAV',
     'AAAAAAAVE',
     'AAAAAAVEA',
     'PQQPPQPPP',
     'QQPPQPPPP',
     'QPPQPPPPP',
     'PPQPPPPPP',
     'PQPPPPPPQ',
     'QPPPPPPQG',
     'PPPPPPQGP',
     'GAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAH',
     'APPGPGGGG',
     'PPGPGGGGG',
     'PGPGGGGGG',
     'GPGGGGGGA',
     'PGGGGGGAG',
     'GGGGGGAGG',
     'GGGGGAGGG',
     'GGGGAGGGA',
     'EHHHHHHHH',
     'HHHHHHHHA',
     'HHHHHHHAH',
     'GGGGGGAGP',
     'GGGGGAGPG',
     'AAAAASGAG',
     'GAGGAAGAG',
     'QQQQRQQQQ',
     'QQQRQQQQQ',
     'QQRQQQQQQ',
     'QRQQQQQQQ',
     'RQQQQQQQQ',
     'QQQQQQQQQ',
     'QQQQQQQQQ',
     'QQQQQQQQE',
     'GGGDSGGGG',
     'GGDSGGGGG',
     'GDSGGGGGG',
     'SGGGGGGRP',
     'SPPPAPAPP',
     'PQPRPPPPP',
     'QPRPPPPPP',
     'PRPPPPPPP',
     'RPPPPPPPG',
     'PPPPPPPGE',
     'APPPQPAPP',
     'RGGGAGGAG',
     'AAPAAPPPA',
     'AAAAAATAA',
     'AAAAATAAA',
     'AAAATAAAA',
     'AAATAAAAA',
     'AATAAAAAS',
     'GAAAGAAGG',
     'PGPPPAGPP',
     'GAAPAAAAG',
     'NAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAG',
     'AAAAAAAGR',
     'LQAPPPPPP',
     'QAPPPPPPG',
     'APPPPPPGP',
     'PPPPPPGPG',
     'PPPPPGPGG',
     'PPPPGPGGP',
     'PPPGPGGPQ',
     'SPAAAAAAA',
     'PAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAS',
     'AAAAAAASA',
     'AAAAAASAP',
     'EEEEDEEDE',
     'GSGGGGGGS',
     'SGGGGGGSS',
     'GGGGGGSSG',
     'GGGGGSSGG',
     'GGGGSSGGG',
     'PAPAPPPGA',
     'APAPPPGAP',
     'SSSASSSAS',
     'SSASSSASS',
     'SASSSASSS',
     'ASSSASSSS',
     'SSSASSSSV',
     'TSGSSSSSS',
     'SGSSSSSSS',
     'GSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSG',
     'SSSSSSSGL',
     'PGPEPPPPP',
     'GPEPPPPPP',
     'PEPPPPPPR',
     'PPPGPPQAP',
     'PPGPPQAPP',
     'DSEEEEEEE',
     'SEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEM',
     'EEEEEEEMA',
     'GLAGAAAAA',
     'LAGAAAAAA',
     'AGAAAAAAA',
     'GAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAE',
     'AAAAAAAEA',
     'AAAAAAEAG',
     'GGGNGGGGG',
     'AAAAAAASA',
     'AAAAAASAL',
     'ATSAAAAAA',
     'TSAAAAAAA',
     'SAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAG',
     'AAAAAAAGS',
     'SGGGGGGSR',
     'GGGGGGSRS',
     'GGRGGRGRG',
     'GRGGRGRGG',
     'RGGRGRGGG',
     'GGRGRGGGR',
     'GGYGGGGGG',
     'GYGGGGGGG',
     'KSGGGGGGG',
     'SGGGGGGGG',
     'GGGGGGGGS',
     'GGGGGGGSS',
     'GGGGGYGGS',
     'PEPAPAPPP',
     'EPAPAPPPP',
     'PAPAPPPPG',
     'APAPPPPGA',
     'QQEEEEEEE',
     'QEEEEEEED',
     'EEEEEEEDS',
     'REEEEGEEE',
     'EEEEGEEEK',
     'EEEGEEEKE',
     'GLEEEEEEE',
     'LEEEEEEEA',
     'EEEEEEEAS',
     'LLALLALLL',
     'LALLALLLL',
     'PEEEKEEEE',
     'EEEKEEEEL',
     'ALALAALAA',
     'QQLLQQQQQ',
     'QLLQQQQQQ',
     'LQQQQQQTQ',
     'GPALPPPPP',
     'PALPPPPPP',
     'ALPPPPPPP',
     'LPPPPPPPL',
     'PPPPPPPLP',
     'PPPPPPLPA',
     'PPPPPLPAA',
     'PPPPLPAAP',
     'PPPLPAAPP',
     'AEPPPPPPP',
     'EPPPPPPPP',
     'PPPPPPPPP',
     'PPPPPPPPP',
     'PPPPPPPPA',
     'PPPPPPPAL',
     'PPPPPPALP',
     'PPPPPALPG',
     'PPPPALPGP',
     'SAASSSSSS',
     'PVAAAAAAA',
     'VAAAAAAAG',
     'AAAAAAAGK',
     'PQGPPPPPQ',
     'GPPPPPQGG',
     'PPPPPPSPG',
     'PPPPPSPGA',
     'PLTPPPPPP',
     'LTPPPPPPP',
     'TPPPPPPPS',
     'PPPPPPPSR',
     'GGPPPPPPP',
     'GPPPPPPPP',
     'PPPPPPPPP',
     'PPPPPPPPR',
     'PPPPPPPRA',
     'PPPPPPRAS',
     'SSPSNSSSS',
     'SPSNSSSSS',
     'SNSSSSSDS',
     'NSSSSSDSS',
     'SSSSSDSSS',
     'SSSSDSSSD',
     'SSSDSSSDS',
     'SSDSSSDSD',
     'PPPPQKPPP',
     'PPPQKPPPP',
     'EEGEEDEDE',
     'AAAAAAGAA',
     'AAAAAGAAS',
     'AAAAGAASG',
     'SAAALAAAA',
     'AAALAAAAT',
     'EDQEEEEEE',
     'DQEEEEEEE',
     'QEEEEEEEE',
     'EEEEEEEED',
     'EEEEEEEDG',
     'SEEEEEEKE',
     'EEEEEEKEE',
     'EEEEEKEEE',
     'EEEEKEEEE',
     'EEEKEEEEE',
     'EEKEEEEEE',
     'EKEEEEEEE',
     'KEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEED',
     'EEEEEEEDE',
     'EEEEEEDEN',
     'KKKKKGKKG',
     'KKKKGKKGK',
     'KKKGKKGKK',
     'DSSSSGSGS',
     'SSSSGSGSD',
     'EEEELTEEE',
     'GSSASSSSG',
     'PLLLLLLLL',
     'LLLLLLLLG',
     'LLLLLLLGG',
     'GGGPPPPPA',
     'GPPPPPALP',
     'PPAPPAGGP',
     'PAPPAGGPP',
     'APPAGGPPP',
     'PPAGGPPPP',
     'PAGGPPPPP',
     'AGGPPPPPG',
     'GGPPPPPGP',
     'GPPPPPGPP',
     'PPPPPGPPP',
     'PPPPGPPPP',
     'PPPGPPPPP',
     'PPGPPPPPG',
     'PGPPPPPGP',
     'GPPPPPGPP',
     'PPPPPGPPP',
     'PPPPGPPPP',
     'PPPGPPPPP',
     'PPGPPPPPG',
     'PGPPPPPGL',
     'GPPPPPGLP',
     'PPPPPGLPP',
     'PPPPGLPPS',
     'GGGPPPAPP',
     'GPPPAPPLP',
     'PPPAPPLPA',
     'GPGGGGAGA',
     'PGGGGAGAP',
     'GGGGAGAPG',
     'PSLLLLLLL',
     'SLLLLLLLP',
     'LLLLLLLPG',
     'APGGGGGGG',
     'PGGGGGGGG',
     'GGGGGGGGV',
     'GGGGGGGVP',
     'GGGGGGVPG',
     'QPPPLPPPP',
     'PPPLPPPPP',
     'PPLPPPPPP',
     'PLPPPPPPA',
     'LPPPPPPAP',
     'PPPPPPAPG',
     'PPPPPAPGS',
     'PPAQAPPPP',
     'APPPPQQPP',
     'PLLPPPPPP',
     'LPPPPPPQG',
     'PPPPPPQGS',
     'TGGGGGGPG',
     'GGGGGGPGL',
     'GSGGSGGGS',
     'GLPGPPGPP',
     'LPGPPGPPG',
     'LPPPLPPGP',
     'PPPLPPGPP',
     'PPLPPGPPS',
     'AVLLLLLLL',
     'VLLLLLLLS',
     'LLLLLLLSL',
     'LLLLLLSLA',
     'LLLLLSLAL',
     'PRAGGGGGG',
     'EETEKEEEE',
     'ETEKEEEEE',
     'EEEEEDREE',
     'EEEEDREEE',
     'EEEDEDEVE',
     'EDEVEDEEE',
     'QGPPGPPGP',
     'TSPSSSSSS',
     'SPSSSSSSS',
     'PSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSG',
     'SSSSSSSGF',
     'APPAPPSPA',
     'PSPPPSPPP',
     'SPPPSPPPS',
     'PPPSPPPSP',
     'PPSPPPSPA',
     'PSPPPSPAP',
     'STSSSASST',
     'PASSTSSSS',
     'LLWLLLLLL',
     'KSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSE',
     'SSSSSSSES',
     'SSSSSSESN',
     'SGGGAGGPG',
     'GGGAGGPGA',
     'GGAGGPGAG',
     'GAGGPGAGG',
     'CAGGGGGGG',
     'AGGGGGGGG',
     'GGGGGGGGA',
     'GGGGGGGAA',
     'GGGGGGAAP',
     'HHHHNHHHH',
     'HHHNHHHHH',
     'SCSSSPSSS',
     'CSSSPSSSS',
     'SSSPSSSSS',
     'SSPSSSSSE',
     'SPSSSSSES',
     'PSSSSSESS',
     'SSSSSESSE',
     'TPSSSSSSS',
     'PSSSSSSSS',
     'SSSSSSSSP',
     'SSSSSSSPS',
     'SSSSSSPSQ',
     'PPAPPARPP',
     'PAPPARPPP',
     'PPARPPPGP',
     'LPLLLLPLL',
     'AQPLPPPPP',
     'PPPPPSQSP',
     'VPPAPPPPP',
     'PPAPPPPPP',
     'PAPPPPPPP',
     'APPPPPPPT',
     'PPPPPPPTS',
     'QESSSSSSS',
     'ESSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSST',
     'SSSSSSSTI',
     'GEKEEEEEE',
     'EKEEEEEEE',
     'KEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEI',
     'AHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHQ',
     'AGAGGAGAA',
     'GAGGAGAAA',
     'AGGAGAAAG',
     'GGAGAAAGG',
     'GAGAAAGGG',
     'AGAAAGGGG',
     'GAAAGGGGA',
     'GPGGGGGPG',
     'PGGGGGPGG',
     'GGGGGPGGG',
     'GGGGPGGGG',
     'GGGPGGGGG',
     'GGPGGGGGP',
     'GPGGGGGPG',
     'PGGGGGPGG',
     'GGGGGPGGG',
     'GGGGPGGGP',
     'GGGPGGGPG',
     'GGPGGGPGG',
     'GPGGGPGGG',
     'PGGGPGGGG',
     'GGGPGGGGG',
     'GGPGGGGGG',
     'GPGGGGGGG',
     'PGGGGGGGP',
     'GGGGGGGPG',
     'GGGGGGPGG',
     'GGGGGPGGG',
     'GGGGPGGGG',
     'GGGPGGGGG',
     'GGPGGGGGG',
     'GPGGGGGGP',
     'PGGGGGGPG',
     'GGGGGGPGG',
     'GGGGGPGGG',
     'GGGGPGGGL',
     'GAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAG',
     'AAAAAAAGQ',
     'VAAASAAAA',
     'AAASAAAAV',
     'GGGGGPAPA',
     'AAAASVAAA',
     'AAASVAAAA',
     'LGLVLLLLL',
     'GLVLLLLLL',
     'LVLLLLLLS',
     'VLLLLLLSL',
     'LLLLLLSLL',
     'LLLVLLLLG',
     'KTEEEEEEE',
     'TEEEEEEEE',
     'EEEEEEEET',
     'EEEEEEETA',
     'EEEEEETAE',
     'TPPLPPPPP',
     'PPLPPPPPT',
     'PLPPPPPTP',
     'LPPPPPTPP',
     'PPPPPTPPG',
     'QPPLPPPPP',
     'PPLPPPPPP',
     'PRPPPPRPG',
     'PPPRPAPSP',
     'GGPGGGAGG',
     'GPGGGAGGA',
     'PGGGAGGAG',
     'GGGAGGAGG',
     'GGAGGAGGG',
     'GAGGAGGGR',
     'EPPPPPPLP',
     'PPPPPPLPL',
     'PPPPPLPLA',
     'PPPLPASPP',
     'TTITTTTTS',
     'AAPPPAAPP',
     'APPPAAPPA',
     'PPPAAPPAS',
     'GASSSSSSS',
     'ASSSSSSST',
     'SSSSSSSTK',
     'HPHPHHHHH',
     'PHPHHHHHH',
     'HPHHHHHHH',
     'PHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHA',
     'AAVAVAAAA',
     'AVAVAAAAA',
     'SSSSDSSDS',
     'SSSDSSDSD',
     'DDDDDDDDG',
     'VESEEEEEE',
     'ESEEEEEES',
     'SLASSSSSS',
     'LASSSSSSS',
     'ASSSSSSSS',
     'SSSSSSSSA',
     'SSSSSSSAV',
     'TTTTTTTAT',
     'TTTTTTATP',
     'LLLVLALLL',
     'ARAAALAAA',
     'AAALAAAVA',
     'EQGEEEEEE',
     'QGEEEEEEE',
     'GEEEEEEED',
     'EEEEEEEDE',
     'EEEEEEDEE',
     'EEEEEDEEE',
     'EEEEDEEEE',
     'EEEDEEEEE',
     'EEDEEEEEK',
     'ASEEEEEEE',
     'SEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEED',
     'EEEEEEEDE',
     'EEEEEEDED',
     'EEEEEDEDE',
     'EEEEDEDEE',
     'EEEDEDEEE',
     'EEDEDEEEE',
     'EDEDEEEEV',
     'RRRRRPRRR',
     'RRRRPRRRR',
     'RRRPRRRRR',
     'PGPLGPPGP',
     'GPPGPPGPP',
     'PPGPPGPPR',
     'ATPPPPPPP',
     'TPPPPPPPT',
     'TPPPPPPRP',
     'PPPPPPRPP',
     'PPPPPRPPS',
     'GAGGPGPGG',
     'SISSSSSTS',
     'ISSSSSTSS',
     'SSSSSTSSD',
     'SSSSTSSDS',
     'SSSTSSDSS',
     'SSDSSDSSS',
     'SDSSDSSSS',
     'DSSDSSSSS',
     'SSDSSSSSS',
     'SDSSSSSSD',
     'DSSSSSSDD',
     'SSSSSSDDS',
     'LPPPPQPAP',
     'PPPLLPPPG',
     'LPALLLLLL',
     'NGGGGGGGG',
     'GGGGGGGGS',
     'GGGGGGGSG',
     'GGGGGGSGL',
     'GNGGGGGGG',
     'GGGGGGGPR',
     'GSSSSTSST',
     'SSSSTSSTA',
     'SSSTSSTAS',
     'SVTSSSSSS',
     'VTSSSSSSS',
     'TSSSSSSSD',
     'SSSSSSSDS',
     'SSSSSSDSS',
     'SSSSSDSSA',
     'SSSSDSSAS',
     'DESSSSSSS',
     'ESSSSSSSS',
     'SSSSSSSSA',
     'SSSSSSSAS',
     'SSSSSSASS',
     'SSSSSASST',
     'SSSSASSTT',
     'SSSASSTTS',
     'SSASSTTSS',
     'SASSTTSSS',
     'ASSTTSSSS',
     'SSTTSSSSS',
     'SSSSSDSDS',
     'SSSSDSDSD',
     'SSSDSDSDS',
     'SSDSDSDSS',
     'SDSDSDSSS',
     'DSDSDSSSS',
     'SDSDSSSSS',
     'DSDSSSSSS',
     'SDSSSSSSS',
     'DSSSSSSSS',
     'SSSSSSSST',
     'SSSSSSSTS',
     'SSSSSSTST',
     'DSSSSSSES',
     'SSSSSSESD',
     'EEEDMEEEE',
     'DDDDDDDDE',
     'DDDDDDDEE',
     'DDDDDDEED',
     'GPGPPAAPP',
     'PGPPAAPPP',
     'GPPAAPPPA',
     'PGSSSSSSS',
     'GSSSSSSSD',
     'SSSSSSSDE',
     'EKEKKKKKK',
     'KEKKKKKKG',
     'LLLKLLLLL',
     'LLKLLLLLL',
     'LKLLLLLLL',
     'KLLLLLLLL',
     'LLLLLLLLL',
     'LLLLLLLLL',
     'LLLLLLLLL',
     'LLLLLLLLA',
     'LLLLLLLAL',
     'LLLLLLALF',
     'LLLLLALFL',
     'HPHHHHHPH',
     'PHHHHHPHH',
     'HHHHHPHHH',
     'HHHHPHHHH',
     'HHHPHHHHH',
     'HHPHHHHHH',
     'HPHHHHHHH',
     'HHHHHHHPP',
     'HPPQPPPPP',
     'PPQPPPPPP',
     'PQPPPPPPP',
     'QPPPPPPPP',
     'PPPPPPPPP',
     'PPPPPPPPP',
     'PPPPPPPPH',
     'EAAAVAAAA',
     'AAAVAAAAA',
     'AAVAAAAAA',
     'AVAAAAAAA',
     'VAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAV',
     'AAAAAAAVG',
     'GSAAAAAAA',
     'SAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAS',
     'AAAAAAAST',
     'SVAAAAAAA',
     'VAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAG',
     'AAAAAAAGV',
     'LTLLLLLLL',
     'LLLLLLLGL',
     'GGSGGSGGS',
     'VTAAAAAAA',
     'TAAAAAAAA',
     'AAAAAAAAS',
     'AAAAAAASG',
     'AAAAAASGA',
     'AAAAASGAA',
     'AAAASGAAA',
     'AAASGAAAA',
     'AASGAAAAA',
     'SAASAAAAA',
     'AASAAAAAA',
     'ASAAAAAAA',
     'SAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAS',
     'AAAAAAASS',
     'AAAAAASSA',
     'AAAAASSAA',
     'AAAASSAAA',
     'AAASSAAAA',
     'QSAAAAAAA',
     'SAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAA',
     'AAAAAAAAL',
     'AAAAAAALG',
     'GSRGGGGGG',
     'SRGGGGGGG',
     'RGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGG',
     'GGGGGGGGA',
     'GGGGGGGAG',
     'GGGGGGAGA',
     'GGGGGAGAG',
     'GGGGAGAGG',
     'GGGAGAGGG',
     'GGAGAGGGS',
     'KEKKEKKKK',
     'ESDDDDDDD',
     'SDDDDDDDD',
     'ASLLLLLLL',
     'LLLLLLLFA',
     'PGPPLQPPP',
     'GPPLQPPPP',
     'PPLQPPPPA',
     'PLQPPPPAP',
     'PLLLLLLLL',
     'LLLLLLLLP',
     'APPPPPPRP',
     'PPPPPPRPQ',
     'PPPPPRPQP',
     'LMPPPPPPP',
     'MPPPPPPPG',
     'PPPPPPPGP',
     'PPPPPPGPM',
     'SEEEEDEEE',
     'EEEEDEEEE',
     'EEEDEEEEE',
     'EEDEEEEEQ',
     'AAEAAAAPA',
     'PASSPSSSS',
     'SSPSPPPPP',
     'SPSPPPPPP',
     'PSPPPPPPP',
     'SPPPPPPPV',
     'PLPPPPPPL',
     'LPPPPPPLP',
     'PPPPPPLPS',
     'PPPPPLPSP',
     'PPPPLPSPG',
     'LPALLLLLL',
     'PALLLLLLL',
     'PHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHH',
     'HHHHHHHHP',
     'HHHHHHHPH',
     'HHHHHHPHH',
     'HHHHHPHHH',
     'SGNSSSSSS',
     'SSSSGGSSS',
     'QEEEDEEEE',
     'EEEDEEEEE',
     'EEDEEEEEE',
     'EDEEEEEEE',
     'DEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEK',
     'EEEEEEEKE',
     'EEEEEEKEE',
     'EEEEEKEEE',
     'EEEEKEEEE',
     'EEEKEEEEE',
     'SSSSSGESS',
     'PEEEEEEGE',
     'EEEEEEGEE',
     'EEEEEGEEE',
     'EEEEGEEEE',
     'EEEGEEEEE',
     'EEGEEEEED',
     'EGEEEEEDD',
     'GEEEEEDDE',
     'EEEEEDDEE',
     'EEEEDDEEE',
     'EEEDDEEED',
     'KSKKKKKKS',
     'KKKSKKKKK',
     'KKSKKKKKK',
     'KSKKKKKKK',
     'SKKKKKKKS',
     'KSKKKKKKS',
     'EAVAAAAAA',
     'AVAAAAAAS',
     'VAAAAAASA',
     'PGPPGQPGP',
     'QEEEMEEEE',
     'EEEMEEEEE',
     'EEMEEEEEE',
     'EMEEEEEEE',
     'MEEEEEEEE',
     'EEEEEEEEG',
     'EEEEEEEGE',
     'EEEEEEGEA',
     'GPPRGGPGG',
     'AVLLLLLLL',
     'LLLLLLLPP',
     'LLLLLLPPL',
     'LLLLLPPLL',
     'LLLLPPLLL',
     'LLLPPLLLL',
     'PGPGPGPGP',
     'LLLLLPSLL',
     'LLLLPSLLL',
     'LLLPSLLLL',
     'LLPSLLLLL',
     'LPSLLLLLL',
     'PSLLLLLLL',
     'SLLLLLLLP',
     'LLLLLLLPG',
     'APPGAAPAP',
     'STDSSSSSS',
     'DSSSSSSES',
     'SSSSSSESD',
     'KKKKKKKGG',
     'GRRRRRRRR',
     'RRRRRRRRR',
     'RRRRRRRRR',
     'RRRRRRRRR',
     'RRRRRRRRS',
     'RRRRRRRSR',
     'RRRRRRSRS',
     'RRRRRSRSS',
     'GSSSGGSSS',
     'SSSGGSSSG',
     'SSGGSSSGS',
     'SGGSSSGSS',
     'GGSSSGSSS',
     'GSSSGSSSG',
     'SSSGSSSGG',
     'SSGSSSGGS',
     'SGSSSGGSS',
     'GSSSGGSSG',
     'SSSGGSSGG',
     'SSGGSSGGS',
     'SGGSSGGSS',
     'GGSSGGSSG',
     'GSSGGSSGG',
     'SSGGSSGGS',
     'SGGSSGGSS',
     'GGSSGGSSG',
     'GAAGGGGGG',
     'AAGGGGGGS',
     'AGGGGGGSG',
     'GGGGGGSGG',
     'GGGGGSGGG',
     'GGGGSGGGG',
     'GGGSGGGGT',
     'GGSGGGGTG',
     'GSGGGGTGG',
     'GGGGTGGSG',
     'DSGSSSSSS',
     'SGSSSSSSS',
     'GSSSSSSSS',
     'SSSSSSSSS',
     'SSSSSSSSF',
     'EPAAAAAAA',
     'PAAAAAAAA',
     'AAAAAAAAI',
     'KREEEEEEE',
     'REEEEEEEA',
     'EEEEEEEAS',
     'EEEEEEASE',
     'GLLLLLLLL',
     'LLLLLLLLL',
     'LLLLLLLLT',
     'GRAAAAAAA',
     'RAAAAAAAR',
     'AAAAAAARG',
     'APPPALPPP',
     'PPPALPPPP',
     'PPALPPPPR',
     'PALPPPPRP',
     'LSSSSSLSS',
     'SSSSSLSSH',
     'ETSEEEEEE',
     'TSEEEEEEE',
     'SEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEE',
     'EEEEEEEEG',
     'EEEEEEEGE',
     'EEEEEEGEG',
     'EEEEEGEGE',
     'SEEDEEEEE',
     'EEDEEEEEE',
     'EDEEEEEEE',
     'DEEEEEEEG',
     'EEEEEEEGE',
     'EEEEEEGEE',
     'EEEEEGEED',
     'EEEEGEEDE',
     'EEEGEEDEE',
     'KSSPSSSSS',
     'SSPSSSSSA',
     'SPSSSSSAS',
     'PSSSSSASS',
     'SSSSSASSS',
     'SSSSASSSS',
     ...]




```python
len([''.join(row) for row in map_numbertoaa(sample_matrices['test'][energies['test'] < -15])])
```




    154




```python
[np.mean(energies[dataset]) for dataset in datasets]
```




    [-2.1713909870793495,
     -1.797020033252432,
     -2.0204479399259125,
     -2.1570303708287377]




```python

```
#### entropy_calculation.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

%load_ext autoreload
%autoreload 2
```


```python
output = True
L = 9
q = naminoacids

proteome = proteome_path('Human')
seed = 1234
prng = np.random.RandomState(seed)
```


```python
params = np.load('data/Human_%i.npz'%L)
hi = params['hi']
Jij = params['Jij']
```


```python
datasets = ['test', 'train', 'model']
sample_matrices = {}
for dataset in datasets:
    sample_matrices[dataset] =  load_matrix('data/%s_matrix_L%i.csv.gz' % (dataset, L))
```


```python
energies = {}
for dataset in datasets:
    energies[dataset] = np.mean([energy_potts(x, hi, Jij) for x in sample_matrices[dataset]])
```


```python
mcmc_kwargs = dict(nsteps=1e6, nsample=10, nburnin=1e3)
```


```python
F = Fpotts_thermodynamic_integration(hi, Jij, integration_intervals=3, mcmc_kwargs=mcmc_kwargs)
```


```python
Suni = np.log(20)*L
'%e'%np.exp(Suni)
```




    '5.120000e+11'




```python
fis = frequencies(sample_matrices['train'], num_symbols=q)
```


```python
Sind_global = L*scipy.stats.entropy(np.mean(fis, axis=0))
'%e'%np.exp(Sind_global)
```




    '2.032950e+11'




```python
def calc_Sind(fis):
    return np.sum(scipy.stats.entropy(fis.T))
```


```python
Sind = calc_Sind(fis)
'%e'%np.exp(Sind)
```




    '2.032795e+11'




```python
Smaxent = np.mean(energies['train']) - F
'%e'%np.exp(Smaxent)
```




    '1.712879e+11'




```python
Sind*np.log2(np.exp(1))/L, Smaxent*np.log2(np.exp(1))/L
```




    (4.1738526076016536, 4.1464036223598955)




```python
df = pd.read_csv('../kmerentropy/data/entropy.csv')
np.array(df['Human'])/np.arange(1, 6)
```




    array([4.17756346, 4.16934288, 4.16129658, 4.14931991, 4.11369582])




```python
(Suni-Sind)*np.log2(np.exp(1)), (Sind-Smaxent)*np.log2(np.exp(1))
```




    (1.3326793855713739, 0.24704086717581905)




```python
L = 9
mi = pd.read_csv('../pfam/data/mutualinformation_nozf.csv', index_col=0, usecols=(1, 2))
mi = np.asarray(mi.loc[:L-2]).flatten()
np.sum(mi*(L-np.arange(1, L)))
```




    0.33534763650457045



# Global model


```python
sample_matrices['model_global'] =  load_matrix('../globalmaxent/data/model_matrix.csv.gz')
params = np.load('../globalmaxent/data/Human_L%i.npz'%L)
h = params['h']
J = params['J']
```


```python
Fcov = Fcov_thermodynamic_integration(h, J, L, integration_intervals=3, mcmc_kwargs=mcmc_kwargs)
```


```python
Scov = np.mean([energy_ncov(x, h, J) for x in sample_matrices['model_global']])-Fcov
'%e'%np.exp(Scov)
```




    '1.790616e+11'




```python
data = np.exp(np.array([Suni, Sind, Scov, Smaxent]))
label = ['Uniform', '1-point', '2nd moment', '2-point']
fig, ax = plt.subplots(figsize=(3.4, 4.0))
pd.Series(data=data, index=label).plot.bar()
#plt.ylim(0,6e11)
plt.grid()
plt.ylabel('Effective Diversity $exp(S)$')
fig.tight_layout()
fig.savefig('entropies.png')
fig.savefig('../../paper/images/diversity_reduction.pdf')
```


![png](notebook_files/entropy_calculation_21_0.png)



```python
(data[1:]-data[:-1])/data[:-1], (data[3]-data[0])/data[0]
```


```python

```
#### distribution_hamming_distances.ipynb

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

from numba import jit

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *
plt.style.use('../peptidome.mplstyle')

from common import model_hierarchy, labels
```


```python
datasets = ['train', 'test', 'flat', 'independent']
datasets.extend(model_hierarchy)
sample_matrices = {}
for dataset in datasets:
    sample_matrices[dataset] =  load_matrix('data/%s_matrix_L9.csv.gz' % dataset)[::9, :]
```


```python
nsample = 2e8
L = sample_matrices['test'].shape[1]
hists_dict = {}
for dataset in datasets:
    hists_dict[dataset] = pairwise_distances_jit(sample_matrices[dataset], N=nsample)
```


```python
hists_dict['train_test'] = pairwise_distances_jit(sample_matrices['train'],
                                                  data2=sample_matrices['test'],
                                                  N=nsample)
```


```python
bins = np.arange(L+1)
fig, ax = plt.subplots()
datasets = ['flat', 'test', 'train_test']
datasets.extend(model_hierarchy)
for dataset in datasets"
    ax.step(bins, hists_dict[dataset], label=dataset, where='mid')
ax.set_yscale('log')
ax.set_xticks(bins)
ax.set_xlim(bins[0], bins[-1])
ax.legend(loc='upper left');
```


      File "<ipython-input-20-e193c107a343>", line 5
        for dataset in datasets"
                                ^
    SyntaxError: EOL while scanning string literal




```python
arr = np.load('../aafreqpca/data/data.npz')
aa_human = arr['human']
ps = aa_human
blocklength = sample_matrices['test'].shape[0]//len(ps)
sampless = []
for p in ps:
    samples = np.random.choice(np.arange(0, 20, 1),
                               size=(blocklength, sample_matrices['test'].shape[1]),
                               p=p)
    sampless.append(samples)
sample_composition = np.concatenate(sampless)
```


```python
hists_dict['composition'] = pairwise_distances_jit(sample_composition, N=nsample)
```


```python
fig, ax = plt.subplots()
datasets = ['flat', 'independent']
datasets.extend(model_hierarchy)
for dataset in datasets:
    ax.errorbar(bins, (hists_dict[dataset])/(hists_dict['train_test']),
                hists_dict[dataset]**.5/hists_dict['train_test'],
                label=labels[dataset])
ax.set_xticks(bins)
ax.axhline(1.0, c='k')
ax.set_ylim(0.0, 1.1)
ax.legend(loc='lower right')
ax.set_xlabel('Hamming distance')
ax.set_ylabel('Fraction relative to train | test')
fig.tight_layout()
```


```python
pairwise_hist = {}
for dataset in datasets:
    pairwise_hist[dataset] = pairwise_distances_jit(sample_matrices[dataset],
                                   data2=sample_matrices['test'],
                                   N=nsample)
```


```python
fig, ax = plt.subplots()
ax.step(bins, hists_dict['test'], where='mid', label='data|data')
for dataset in datasets:
    ax.step(bins, pairwise_hist[dataset], where='mid', label=dataset + '|data')
ax.set_yscale('log')
ax.set_xticks(bins);
ax.legend(loc='upper left')
ax.set_xlabel('Hamming distance')
ax.set_ylabel('Count');
```


```python
fig, ax = plt.subplots()
for dataset in datasets:
    ax.errorbar(bins, pairwise_hist[dataset]/hists_dict['train_test'],
                pairwise_hist[dataset]**.5/hists_dict['train_test'],
                label=labels[dataset] + ' | test')
ax.set_xticks(bins)
ax.legend(loc='lower right')
ax.axhline(1.0, c='k')
ax.set_xlabel('Hamming distance')
ax.set_ylabel('Fraction relative to train | test')
fig.tight_layout()
fig.savefig('../../paper/images/hammingdist.pdf')
```


```python

```
#### cijk_inspection.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *
import xarray as xr
```


```python
observables = ['fi', 'fij', 'cij', 'fijk', 'cijk', 'fold_ijk']#, 'fijkl']
observables_dict = {key: dict() for key in observables}
datasets = ['train', 'test', 'model_nskewfcov']
for dataset in datasets:
    params = np.load('data/%s_observables.npz'%dataset)
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
```


```python
L = observables_dict['fi']['test'].shape[0]
```


```python
#for i in range(L):
#    for j in range(L):
#        for k in range(L):
#            if (i == j) or (j == k) or (i==k):
#                observables_dict['cijk']['test'][i,j,k] = 0.0
for i in range(L):
    for j in range(L):
        for k in range(L):
            if not ((i < j) and (j < k)):
                for dataset in datasets:
                    observables_dict['cijk'][dataset][i,j,k] = 0.0
```


```python
cijk_xr = xr.DataArray(1e4*(observables_dict['cijk']['test']-observables_dict['cijk']['model_nskewfcov']),
             dims=("i", "j", 'k', 'alpha', 'beta', 'gamma'),
             coords={'i': range(L),
                     'j': range(L),
                     'k': range(L),
                     'alpha': list(aminoacids),
                     'beta':list(aminoacids),
                     'gamma':list(aminoacids)})
```


```python
cijk_df = cijk_xr.to_dataframe('cijk')
cijk_df_flat = cijk_df.loc[0].reset_index()
cijk_df_flat.sort_values('cijk').tail(n=30)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>j</th>
      <th>k</th>
      <th>alpha</th>
      <th>beta</th>
      <th>gamma</th>
      <th>cijk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>169263</th>
      <td>2</td>
      <td>3</td>
      <td>E</td>
      <td>E</td>
      <td>E</td>
      <td>1.086662</td>
    </tr>
    <tr>
      <th>358315</th>
      <td>4</td>
      <td>8</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>1.116465</td>
    </tr>
    <tr>
      <th>137389</th>
      <td>1</td>
      <td>8</td>
      <td>E</td>
      <td>L</td>
      <td>L</td>
      <td>1.119897</td>
    </tr>
    <tr>
      <th>250112</th>
      <td>3</td>
      <td>4</td>
      <td>G</td>
      <td>G</td>
      <td>P</td>
      <td>1.178561</td>
    </tr>
    <tr>
      <th>188905</th>
      <td>2</td>
      <td>5</td>
      <td>P</td>
      <td>G</td>
      <td>G</td>
      <td>1.196761</td>
    </tr>
    <tr>
      <th>259789</th>
      <td>3</td>
      <td>5</td>
      <td>L</td>
      <td>L</td>
      <td>L</td>
      <td>1.207989</td>
    </tr>
    <tr>
      <th>90105</th>
      <td>1</td>
      <td>2</td>
      <td>G</td>
      <td>G</td>
      <td>G</td>
      <td>1.211339</td>
    </tr>
    <tr>
      <th>93473</th>
      <td>1</td>
      <td>2</td>
      <td>Q</td>
      <td>Q</td>
      <td>Q</td>
      <td>1.212300</td>
    </tr>
    <tr>
      <th>101915</th>
      <td>1</td>
      <td>3</td>
      <td>R</td>
      <td>S</td>
      <td>S</td>
      <td>1.250656</td>
    </tr>
    <tr>
      <th>428905</th>
      <td>5</td>
      <td>8</td>
      <td>P</td>
      <td>G</td>
      <td>G</td>
      <td>1.292466</td>
    </tr>
    <tr>
      <th>93052</th>
      <td>1</td>
      <td>2</td>
      <td>P</td>
      <td>P</td>
      <td>P</td>
      <td>1.292647</td>
    </tr>
    <tr>
      <th>177263</th>
      <td>2</td>
      <td>4</td>
      <td>E</td>
      <td>E</td>
      <td>E</td>
      <td>1.294225</td>
    </tr>
    <tr>
      <th>98245</th>
      <td>1</td>
      <td>3</td>
      <td>G</td>
      <td>P</td>
      <td>G</td>
      <td>1.330061</td>
    </tr>
    <tr>
      <th>131669</th>
      <td>1</td>
      <td>7</td>
      <td>L</td>
      <td>E</td>
      <td>L</td>
      <td>1.357626</td>
    </tr>
    <tr>
      <th>182315</th>
      <td>2</td>
      <td>4</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>1.374905</td>
    </tr>
    <tr>
      <th>270315</th>
      <td>3</td>
      <td>6</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>1.382944</td>
    </tr>
    <tr>
      <th>170245</th>
      <td>2</td>
      <td>3</td>
      <td>G</td>
      <td>P</td>
      <td>G</td>
      <td>1.415377</td>
    </tr>
    <tr>
      <th>122245</th>
      <td>1</td>
      <td>6</td>
      <td>G</td>
      <td>P</td>
      <td>G</td>
      <td>1.425663</td>
    </tr>
    <tr>
      <th>88000</th>
      <td>1</td>
      <td>2</td>
      <td>A</td>
      <td>A</td>
      <td>A</td>
      <td>1.543841</td>
    </tr>
    <tr>
      <th>91789</th>
      <td>1</td>
      <td>2</td>
      <td>L</td>
      <td>L</td>
      <td>L</td>
      <td>1.544864</td>
    </tr>
    <tr>
      <th>132905</th>
      <td>1</td>
      <td>7</td>
      <td>P</td>
      <td>G</td>
      <td>G</td>
      <td>1.592152</td>
    </tr>
    <tr>
      <th>178105</th>
      <td>2</td>
      <td>4</td>
      <td>G</td>
      <td>G</td>
      <td>G</td>
      <td>1.598509</td>
    </tr>
    <tr>
      <th>498112</th>
      <td>6</td>
      <td>8</td>
      <td>G</td>
      <td>G</td>
      <td>P</td>
      <td>1.660037</td>
    </tr>
    <tr>
      <th>282112</th>
      <td>3</td>
      <td>8</td>
      <td>G</td>
      <td>G</td>
      <td>P</td>
      <td>1.715788</td>
    </tr>
    <tr>
      <th>194245</th>
      <td>2</td>
      <td>6</td>
      <td>G</td>
      <td>P</td>
      <td>G</td>
      <td>1.900700</td>
    </tr>
    <tr>
      <th>348905</th>
      <td>4</td>
      <td>7</td>
      <td>P</td>
      <td>G</td>
      <td>G</td>
      <td>1.907632</td>
    </tr>
    <tr>
      <th>410245</th>
      <td>5</td>
      <td>6</td>
      <td>G</td>
      <td>P</td>
      <td>G</td>
      <td>1.917176</td>
    </tr>
    <tr>
      <th>258112</th>
      <td>3</td>
      <td>5</td>
      <td>G</td>
      <td>G</td>
      <td>P</td>
      <td>1.971702</td>
    </tr>
    <tr>
      <th>108905</th>
      <td>1</td>
      <td>4</td>
      <td>P</td>
      <td>G</td>
      <td>G</td>
      <td>2.024029</td>
    </tr>
    <tr>
      <th>266105</th>
      <td>3</td>
      <td>6</td>
      <td>G</td>
      <td>G</td>
      <td>G</td>
      <td>5.652935</td>
    </tr>
  </tbody>
</table>
</div>




```python

```
#### maxent_analysis.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

import evcouplings.align, evcouplings.couplings
```


```python
output = True
L = 9
q = naminoacids

proteome = proteome_path('Human')
seed = 1234
prng = np.random.RandomState(seed)
```


```python
params = np.load('data/Human_full_k9.npz')
hi = params['hi']
Jij = params['Jij']
```


```python
datasets = ['train', 'test', 'model']
sample_matrices = {}
for dataset in datasets:
    sample_matrices[dataset] =  np.loadtxt('data/%s_matrix_L%g.csv.gz'%(dataset, L)).astype(int)
```


```python
for dataset in datasets:
    print(dataset, sample_matrices[dataset].shape)
```

    train (5421876, 9)
    test (5367489, 9)
    model (5421764, 9)



```python

```


```python
fis = {}
fijs = {}
cijs = {}
for dataset in datasets:
    matrix = sample_matrices[dataset]
    fi = frequencies(matrix, num_symbols=q, pseudocount=0.0)
    fis[dataset] = fi
    fij = pair_frequencies(matrix, num_symbols=q, fi=fi, pseudocount=0.0)
    fijs[dataset] = fij
    cij = compute_covariance_matrix(fi=fi, fij=fij)
    cijs[dataset] = cij
```


```python
fijks = {}
for dataset in datasets:
    matrix = sample_matrices[dataset]
    fijk = triplet_frequencies(matrix, num_symbols=q, pseudocount=0.0)
    fijks[dataset] = fijk
```


```python
cijks = {}
for dataset in datasets:
    matrix = sample_matrices[dataset]
    fij = fijs[dataset]
    fi = fis[dataset]
    fijk = fijks[dataset]
    cijk = compute_cijk(fijk, fij, fi)
    cijks[dataset] = cijk
```


```python
plt.plot(flatten_ijk(cijks['test']), flatten_ijk(cijks['train']), 'o', ms=1)
```




    [<matplotlib.lines.Line2D at 0x7fd0b134d588>]




![png](notebook_files/maxent_analysis_9_1.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.5), ncols=2)
for i, dataset in enumerate(['train', 'model']):
    plotting.density_scatter(flatten_ijk(cijks['test']),
                             flatten_ijk(cijks[dataset]),
                             trans=lambda x: np.log(x+1e-3),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=50, ax=axes[i])
    axes[i].set_ylabel(dataset)
for ax in axes:
    max_ = 1.1*flatten_ijk(cijks['test']).max()
    lims = 1/max_, max_
    ax.plot(lims, lims, 'k-')
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel('test')
    ax.set_xlim(*lims)
    ax.set_ylim(*lims)
fig.tight_layout()
```


![png](notebook_files/maxent_analysis_10_0.png)



```python
jump = lambda x: local_jump(x, q)
x0 = prng.randint(q, size=N)
independent_matrix = mcmcsampler(x0, lambda x: energy_potts(x, hi, np.zeros_like(Jij)), jump, 1e7, nsample=10)
```


```python
fi_independent = frequencies(independent_matrix, num_symbols=q, pseudocount=0.0)
fij_independent = pair_frequencies(independent_matrix, num_symbols=q, fi=fi_independent, pseudocount=0.0)
fijk_independent = triplet_frequencies(independent_matrix, num_symbols=q, pseudocount=0.0)
```


```python
cijk_independent = compute_cijk(fijk_independent, fij_independent, fi_independent)
```


```python
plt.plot(flatten_ijk(cijks['test']), flatten_ijk(cijk_independent), 'o', ms=1)
```




    [<matplotlib.lines.Line2D at 0x7fd0b18072b0>]




![png](notebook_files/maxent_analysis_14_1.png)



```python
foldijks = {}
for dataset in datasets:
    fijk = fijks[dataset]
    fi = fis[dataset]
    fold_ijk = fijk / (fi[:, np.newaxis, np.newaxis, :, np.newaxis, np.newaxis] *
                fi[np.newaxis, :, np.newaxis, np.newaxis, :, np.newaxis] *
                fi[np.newaxis, np.newaxis :, np.newaxis, np.newaxis, :])
    foldijks[dataset] = fold_ijk
```


```python
params = np.load('data/Human_full_k9.npz')
hi = params['hi']
Jij = params['Jij']
```


```python
list(zip(aminoacids, hi.std(axis=0)))
```




    [('A', 0.0035406579075383795),
     ('C', 0.0014893643088148595),
     ('D', 0.0013892198615577393),
     ('E', 0.0009110189563667492),
     ('F', 0.0013412697278855862),
     ('G', 0.0014000822211791095),
     ('H', 0.002127311629294183),
     ('I', 0.0020652620217782598),
     ('K', 0.0026846894372483626),
     ('L', 0.0011654184849189324),
     ('M', 0.026951582205213515),
     ('N', 0.0012834177758636997),
     ('P', 0.0009682044299691245),
     ('Q', 0.0013840474784771529),
     ('R', 0.001142693073953028),
     ('S', 0.0009424551613649941),
     ('T', 0.0011721207550945312),
     ('V', 0.0008010794265122387),
     ('W', 0.0014400674267044922),
     ('Y', 0.0022629432493179095)]




```python
observables = ['fi', 'fij', 'cij', 'fijk', 'cijk', 'fold_ijk']#, 'fijkl']
observables_dict = {key: dict() for key in observables}
for dataset in ['train', 'test', 'model', 'model_global']:
    params = np.load('data/%s_observables.npz'%dataset)
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
```


```python
kmers = np.array([list(kmer) for kmer in
                  to_kmers(pd.read_csv('../pfam/data/human_nozf.csv')['Sequence'], 9)])
filtered = map_matrix(kmers, map_)[::9]
```


```python
matrix = filtered
fi = frequencies(matrix, num_symbols=naminoacids)
fij = pair_frequencies(matrix, num_symbols=naminoacids, fi=fi)
cij = compute_covariance_matrix(fi, fij)
fijk = triplet_frequencies(matrix, num_symbols=naminoacids)
cijk = compute_cijk(fijk, fij, fi)
```


```python
observables_dict['fi']['filtered'] = fi
observables_dict['cij']['filtered'] = cij
observables_dict['cijk']['filtered'] = cijk
```


```python
fig, ax = plt.subplots()
plotting.density_scatter(flatten_ijk(observables_dict['cijk']['test']),
                         flatten_ijk(cijk),
                         norm=colors.LogNorm(vmin=1),
                         s=0.5, bins=50, ax=ax)
ax.set_xlim(-8e-4, 8e-4)
ax.set_ylim(-8e-4, 8e-4)
```




    (-0.0008, 0.0008)




![png](notebook_files/maxent_analysis_22_1.png)



```python
fig, axes = plt.subplots(figsize=(8, 5), ncols=3, nrows=2)

for j, (observable, label, lims, flattener) in enumerate([('fi', '$f_i$', (0, 0.12), np.ravel),
                                               ('cij', '$C_{ij}$', (-0.0035, 0.0035), flatten_ij),
                                               ('cijk', '$C_{ijk}$', (-8e-4, 8e-4), flatten_ijk)]):
    for i, dataset in enumerate(['model', 'train']):
        ax = axes[i, j]
        if observable in ['cij', 'cijk']:
            flatten = flatten_ij if observable == 'cij' else flatten_ijk
            plotting.density_scatter(flatten(observables_dict[observable]['filtered']),
                             flatten(observables_dict[observable][dataset]),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=40, ax=ax)
        else:
            ax.plot(flattener(observables_dict[observable]['filtered']),
                    flattener(observables_dict[observable][dataset]),
                    'o', ms=1)
        
        ax.set_xlabel('test %s'%label)
        ax.set_ylabel('%s %s'%(dataset, label))
        ax.plot(lims, lims, 'k')
        ax.set_xlim(*lims)
        ax.set_ylim(*lims)

for ax in axes[:, 1:].flatten():
    ax.ticklabel_format(style='sci', scilimits=(0,0))

fig.tight_layout()
```


![png](notebook_files/maxent_analysis_23_0.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.5), ncols=2)
label = r'$\frac{P(\sigma_i, \sigma_j, \sigma_k)}{P(\sigma_i)P(\sigma_j)P(\sigma_k)}$'
for i, dataset in enumerate(['model', 'train']):
    plotting.density_scatter(flatten_ijk(observables_dict['fold_ijk']['test']),
                             flatten_ijk(observables_dict['fold_ijk'][dataset]),
                             trans=lambda x: np.log(x),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=50, ax=axes[i])
    axes[i].set_ylabel('%s %s'%(dataset, label))
for ax in axes:
    ax.set_xlabel('test %s'%label)
    
    max_ = 1.1*flatten_ijk(observables_dict['fold_ijk']['test']).max()
    lims = 1/max_, max_
    ax.plot(lims, lims, 'k-')
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlim(*lims)
    ax.set_ylim(*lims)
fig.tight_layout()
fig.savefig('fold.png')
```


![png](notebook_files/maxent_analysis_24_0.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.5), ncols=2)
for i, dataset in enumerate(['model', 'train']):
    indices = 0,4,8
    plotting.density_scatter(observables_dict['fold_ijk']['test'][indices].flatten(),
                             observables_dict['fold_ijk'][dataset][indices].flatten(),
                             trans=lambda x: np.log(x),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=50, ax=axes[i])
    axes[i].set_ylabel(dataset)
for ax in axes:
    max_ = 1.1*observables_dict['fold_ijk']['test'][indices].flatten().max()
    lims = 1/max_, max_
    ax.plot(lims, lims, 'k-')
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel('test')
    ax.set_xlim(*lims)
    ax.set_ylim(*lims)
fig.tight_layout()
```


![png](notebook_files/maxent_analysis_25_0.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.5), ncols=2)
for i, dataset in enumerate(['model', 'train']):
    if dataset == 'model':
        fi = observables_dict['fi']['train']
        fijk_ind = (fi[:, np.newaxis, np.newaxis, :, np.newaxis, np.newaxis] *
                 fi[np.newaxis, :, np.newaxis, np.newaxis, :, np.newaxis] *
                 fi[np.newaxis, np.newaxis :, np.newaxis, np.newaxis, :])
        test = observables_dict['fijk']['test'][3,4,5,:,:,:].flatten()
        data = fijk_ind[3,4,5,:,:,:].flatten()
        print('ind', calc_jsd(test, data))
        axes[i].plot(test, data, 'o', ms=1)
    test = observables_dict['fijk']['test'][3,4,5,:,:,:].flatten()
    data = observables_dict['fijk'][dataset][3,4,5,:,:,:].flatten()
    print(dataset, calc_jsd(test, data))
    axes[i].plot(test, data, 'o', ms=1)
    axes[i].set_ylabel(dataset)
for ax in axes:
    #max_ = 1.1*flatten_ijk(observables_dict['fijk']['test']).max()
    #lims = 1/max_, max_
    #ax.plot(lims, lims, 'k-')
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel('test')
    #ax.set_xlim(*lims)
    #ax.set_ylim(*lims)
fig.tight_layout()
```

    ind 0.0073301421024185645
    model 0.0015892616068755262
    train 0.0004770915667735209



![png](notebook_files/maxent_analysis_26_1.png)



```python
fig, axes = plt.subplots(figsize=(7.5, 3.5), ncols=2)
for i, dataset in enumerate(['model', 'train']):
    plotting.density_scatter(flatten_ijk(observables_dict['cijk']['test']),
                             flatten_ijk(observables_dict['cijk'][dataset]),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=50, ax=axes[i])
    axes[i].set_ylabel(dataset + ' $C_{ijk}$')
for ax in axes:
    lim = 7e-4
    lims = -lim, lim
    ax.plot(lims, lims, 'k-')
    ax.set_xlim(*lims)
    ax.set_ylim(*lims)
    ax.set_xlabel('test $C_{ijk}$')
fig.tight_layout()
```


![png](notebook_files/maxent_analysis_27_0.png)



```python
fig, axes = plt.subplots(figsize=(7.5, 3.5), ncols=2)
for i, dataset in enumerate(['model', 'train']):
    indices = 3,4,5
    plotting.density_scatter(observables_dict['cijk']['test'][indices].flatten(),
                             observables_dict['cijk'][dataset][indices].flatten(),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=50, ax=axes[i])
    axes[i].set_ylabel(dataset + ' $C_{ijk}$')
for ax in axes:
    lim = 5e-4
    lims = -lim, lim
    ax.plot(lims, lims, 'k-')
    ax.set_xlim(*lims)
    ax.set_ylim(*lims)
    ax.set_xlabel('test $C_{ijk}$')
fig.tight_layout()
```


![png](notebook_files/maxent_analysis_28_0.png)



```python

```
#### Jij_analysis.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors
import seaborn as sns

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *
import xarray as xr

from common import *
```


```python
params = np.load('data/Human_reference_9.npz')
hi = params['hi']
Jij = params['Jij']
```


```python
def zero_sum_gauge(J_ij):
    J_ijc = J_ij.copy()
    for i in range(Jij.shape[0]):
        J_ijc[i, i] = 0.0
    J_ij_0 = (J_ijc
              - np.mean(J_ijc, axis=2)[:, :, np.newaxis, :]
              - np.mean(J_ijc, axis=3)[:, :, :, np.newaxis]
              + np.mean(J_ijc, axis=(2,3))[:, :, np.newaxis, np.newaxis])
    return J_ij_0
```


```python
Jij0 = zero_sum_gauge(Jij)
```


```python
mean_Jij = Jij0.mean(axis=(0,1))
```


```python
L = hi.shape[0]
Jij_xr = xr.DataArray(Jij0-mean_Jij,
             dims=("i", "j", 'alpha', 'beta'),
             coords={'i': range(L),
                     'j': range(L),
                     'alpha': list(aminoacids),
                     'beta':list(aminoacids)
                    })
Jij_df = Jij_xr.to_dataframe('Jij').loc[0,:].reset_index()
Jij_df.dropna(inplace=True)
Jij_df.sort_values('Jij').tail(n=10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>j</th>
      <th>alpha</th>
      <th>beta</th>
      <th>Jij</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>602</th>
      <td>1</td>
      <td>M</td>
      <td>D</td>
      <td>0.210068</td>
    </tr>
    <tr>
      <th>645</th>
      <td>1</td>
      <td>P</td>
      <td>G</td>
      <td>0.211406</td>
    </tr>
    <tr>
      <th>462</th>
      <td>1</td>
      <td>E</td>
      <td>D</td>
      <td>0.213289</td>
    </tr>
    <tr>
      <th>1305</th>
      <td>3</td>
      <td>G</td>
      <td>G</td>
      <td>0.217657</td>
    </tr>
    <tr>
      <th>463</th>
      <td>1</td>
      <td>E</td>
      <td>E</td>
      <td>0.218072</td>
    </tr>
    <tr>
      <th>471</th>
      <td>1</td>
      <td>E</td>
      <td>N</td>
      <td>0.228408</td>
    </tr>
    <tr>
      <th>1221</th>
      <td>3</td>
      <td>C</td>
      <td>C</td>
      <td>0.252854</td>
    </tr>
    <tr>
      <th>3161</th>
      <td>7</td>
      <td>W</td>
      <td>C</td>
      <td>0.270712</td>
    </tr>
    <tr>
      <th>2021</th>
      <td>5</td>
      <td>C</td>
      <td>C</td>
      <td>0.298724</td>
    </tr>
    <tr>
      <th>600</th>
      <td>1</td>
      <td>M</td>
      <td>A</td>
      <td>0.328577</td>
    </tr>
  </tbody>
</table>
</div>




```python
fig, axes = plt.subplots(figsize=(3, 8), sharex=True, sharey=True, nrows=L)
for j in range(1, L):
    ax = axes[j-1]
    ax.hist(Jij_df[Jij_df['j']==j]['Jij'], bins=np.arange(-0.4, 1.0, 0.05), histtype='step')
    ax.text(0.95, 0.95, 'd = %i'%j, transform=ax.transAxes, va='top', ha='right')
ax = axes[-1]
ax.hist(mean_Jij.flatten(), bins=np.arange(-0.4, 1.0, 0.05), histtype='step')
ax.text(0.95, 0.95, 'mean', transform=ax.transAxes, va='top', ha='right')
for ax in axes:
    ax.set_yscale('log')
```


![png](notebook_files/Jij_analysis_6_0.png)



```python
for j in range(1, 9):
    plt.scatter(mean_Jij, Jij0[0, j], s=2)
```


![png](notebook_files/Jij_analysis_7_0.png)



```python
dist = np.arange(1, 9)
for i in range(naminoacids):
    plt.plot(dist, Jij0[0, dist][:,i,i]-np.mean(Jij0[0, dist][:,i,i]))
```


![png](notebook_files/Jij_analysis_8_0.png)



```python
symmetrized = np.zeros_like(mean_Jij)
for i in range(naminoacids):
    for j in range(naminoacids):
        symmetrized[i, j] = 0.5*(mean_Jij[i, j] + mean_Jij[j, i])
```


```python
aminoacids_coucke = 'ACFILMVWYPHKRDENQSTG'
```


```python
indices = [aminoacids.index(aa) for aa in aminoacids_coucke]
```


```python
imshow_kwargs = dict(vmin=-0.25, vmax=0.25, cmap='coolwarm')
fig, axes_arr = plt.subplots(figsize=(10, 8), nrows=3, ncols=3)
axes = axes_arr.flatten()
for j in range(1, 9):
    ax = axes[j-1]
    im = ax.imshow((Jij0[0,j]-mean_Jij)[indices, :][:, indices], **imshow_kwargs)
    ax.set_title(j)
    set_aminoacidslabel(ax, aminoacids_coucke)
    fig.colorbar(im, ax=ax, shrink=0.7)
ax = axes[-1]
im = ax.imshow(mean_Jij[indices, :][:, indices], **imshow_kwargs)
ax.set_title('mean')
label(ax, aminoacids_coucke)
fig.colorbar(im, ax=ax, shrink=0.7)
fig.tight_layout()
fig.savefig('Jij_vs_dist.png')
fig.savefig('../../paper/images/Jij_vs_dist.pdf')
```


![png](notebook_files/Jij_analysis_12_0.png)



```python
for j in range(1, L):
    sns.regplot(mean_Jij.flatten(), (Jij0[0,j]-mean_Jij).flatten(), label=j)
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fd2e1c91320>




![png](notebook_files/Jij_analysis_13_1.png)



```python
fig, axes_arr = plt.subplots(figsize=(9, 8), nrows=2, ncols=2)
axes = axes_arr.flatten()
U, s, Vh = scipy.linalg.svd(Jij0[0, 2]-mean_Jij)
for i in range(4):
    im = axes[i].imshow((s[i]*np.outer(U.T[i], Vh[i]))[indices, :][:, indices], **imshow_kwargs)
    axes[i].set_title(i+1)
    label(axes[i], aminoacids_coucke)
    fig.colorbar(im, ax=axes[i], shrink=0.7)
fig.tight_layout()
```


![png](notebook_files/Jij_analysis_14_0.png)



```python
w, v = scipy.linalg.eigh(symmetrized)
```


```python
plt.hist(w, bins=np.arange(-0.2, 0.9, 0.05))
```




    (array([0., 0., 1., 1., 3., 3., 3., 3., 0., 1., 2., 0., 1., 0., 0., 0., 0.,
            1., 1., 0., 0.]),
     array([-2.00000000e-01, -1.50000000e-01, -1.00000000e-01, -5.00000000e-02,
            -5.55111512e-17,  5.00000000e-02,  1.00000000e-01,  1.50000000e-01,
             2.00000000e-01,  2.50000000e-01,  3.00000000e-01,  3.50000000e-01,
             4.00000000e-01,  4.50000000e-01,  5.00000000e-01,  5.50000000e-01,
             6.00000000e-01,  6.50000000e-01,  7.00000000e-01,  7.50000000e-01,
             8.00000000e-01,  8.50000000e-01]),
     <a list of 21 Patch objects>)




![png](notebook_files/Jij_analysis_16_1.png)



```python
arr = np.load('../aafreqpca/data/data.npz')
aa_human = arr['human']
```


```python
cov = np.cov(aa_human.T)
```


```python
fig, axes = plt.subplots(figsize=(9, 4), ncols=2)
for i, matrix in enumerate([cov, mean_Jij]):
    ax = axes[i]
    im = ax.imshow(matrix[indices, :][:, indices], vmin=-matrix.max(), vmax=matrix.max(), cmap='coolwarm')
    label(ax, aminoacids_coucke)
    fig.colorbar(im, ax=ax, shrink=0.7)
axes[0].set_title('Covariance at protein level')
axes[1].set_title('mean Jij')
fig.tight_layout()
fig.savefig('meanJij_aacovariance.png')
fig.savefig('../../paper/images/meanJij_aacovariance.pdf')
```


![png](notebook_files/Jij_analysis_19_0.png)



```python

```
#### meanfield.ipynb

```python
import itertools, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

import evcouplings.couplings
```


```python
params = np.load('data/Human_9.npz')
hi = params['hi']
Jij = params['Jij']
```


```python
L, q = hi.shape
L, q
```




    (9, 20)




```python
observables = ['fi', 'fij', 'cij', 'fijk', 'cijk', 'fold_ijk']#, 'fijkl']
observables_dict = {key: dict() for key in observables}
for dataset in ['train', 'test', 'model']:
    params = np.load('data/%s_observables.npz'%dataset)
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
```


```python
@numba.jit(nopython=True)
def _flatten_index(i, alpha, q):
    """
    Map position and symbol to index in
    the covariance matrix.
    Parameters
    ----------
    i : int, np.array of int
        The alignment column(s).
    alpha : int, np.array of int
        The symbol(s).
    q : int
        The number of symbols of the
        alphabet used.
    """
    return i * (q - 1) + alpha
```


```python
@numba.jit(nopython=True)
def reshape_invcov_to_4d(inv_cov_matrix, L, q):
    """
    "Un-flatten" flattened covariance matrix
    
    Parameters
    ----------
    inv_cov_matrix : np.array
        The matrix to be flattened
    L : int
        Model length.
    q : int
        Number of characters in the alphabet.
    Returns
    -------
    np.array
        Matrix of size L x L x
        q x q.
    """
    inv = np.zeros((L, L, q, q))
    for i in range(L):
        for j in range(L):
            for alpha in range(q - 1):
                for beta in range(q - 1):
                    inv[i, j, alpha, beta] = inv_cov_matrix[
                        _flatten_index(i, alpha, q),
                        _flatten_index(j, beta, q)
                    ]
    return inv
```


```python
def compute_Jij_mf(cij_flat):
    invC = np.linalg.inv(cij_flat)
    Jij = -reshape_invcov_to_4d(invC, L, q)
    return Jij
```


```python
@numba.jit(nopython=True)
def fields(J_ij, f_i):
    """
    Compute fields in q gauge.
    
    Parameters
    ----------
    J_ij : np.array
        Matrix of size L x L x q x q
        containing coupling parameters.
    f_i : np.array
        Matrix of size L x q
        containing single-site frequencies.
    Returns
    -------
    np.array
        Matrix of size L x q
        containing single-site fields.
    """
    L, q = f_i.shape
    hi = np.zeros((L, q))
    for i in range(L):
        log_fi = np.log(f_i[i] / f_i[i, q-1])
        J_ij_sum = np.zeros((1, q))
        for j in range(L):
            if i != j:
                # some eij values over beta from 1 to q-1
                J_ij_sum += np.dot(J_ij[i, j, :, :q-1], f_i[j, :q-1]).T
        hi[i] = log_fi - J_ij_sum
    return hi
```


```python
fi = observables_dict['fi']['train']
fij = observables_dict['fij']['train']
alpha = 1e-8
fi_reg = (1-alpha)*fi + alpha/q
fij_reg = (1-alpha)*fij + alpha/q**2
cij_flat = compute_flattened_covariance_matrix(fi_reg, fij_reg)
Jij_mf = compute_Jij_mf(cij_flat)
hi_mf = fields(Jij_mf, fi_reg)
```

    <ipython-input-8-c53c1d109231>:28: NumbaPerformanceWarning: [1m[1mnp.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 1d, C))[0m[0m
      J_ij_sum += np.dot(J_ij[i, j, :, :q-1], f_i[j, :q-1]).T



```python
L, q = fi.shape
rhoij = np.zeros(fij.shape)
for i in range(L):
    for j in range(L):
        rhoij[i, j] = fij[i, j]/np.outer(fi[i], fi[j]) - 1.0
plt.plot(flatten_ij(rhoij), flatten_ij(zero_sum_gauge(Jij)),
         '.', ms=1, label='fitted')
plt.plot(flatten_ij(rhoij), flatten_ij(zero_sum_gauge(Jij_mf)),
         '.', ms=1, label='mf')
lim = 1.0
plt.plot([-lim, lim], [-lim, lim], 'k-')
plt.xlabel(r'$\rho_{ij}$')
plt.ylabel('$J_{ij}$')
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fac0de03d68>




![png](notebook_files/meanfield_9_1.png)



```python
bins = np.linspace(-1, 1, 20)
plt.hist(flatten_ij(zero_sum_gauge(Jij)), bins=bins, histtype='step')
plt.hist(flatten_ij(zero_sum_gauge(Jij_mf)), bins=bins, histtype='step');
```


![png](notebook_files/meanfield_10_0.png)



```python
Jij_mf_zero = zero_sum_gauge(Jij_mf)
hi_mf_zero = fields(Jij_mf_zero, fi)
```


```python
prng = np.random
jump = lambda x: local_jump(x, q)
x0 = prng.randint(q, size=L)
sample_matrix = mcmcsampler(x0, lambda x: energy_potts(x, hi_mf, Jij_mf), jump, 1e6, nsample=10, nburnin=1e4)
```


```python
fi_mf = frequencies(sample_matrix, num_symbols=q)
fij_mf = pair_frequencies(sample_matrix, num_symbols=q, fi=fi_mf)
cij_mf = compute_covariance_matrix(fi_mf, fij_mf)
```


```python
fig, axes = plt.subplots(figsize=(7, 3.2), ncols=2)

ax = axes[0]
lims = [0, 0.12]
ax.plot(np.ravel(fi), np.ravel(fi_mf), 'o', ms=2)
ax.set_xlabel('training $f_i$')
ax.set_ylabel('mf $f_i$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)

ax = axes[1]
lims = [-0.3e-2, 1.5e-2]
plotting.density_scatter(flatten_ij(observables_dict['cij']['train']),
                         flatten_ij(cij_mf),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=30, ax=ax)
#ax.plot(flatten_ij(observables_dict['cij']['train']), flatten_ij(cij_mf), 'o', ms=1)
ax.set_ylabel('mean-field $C_{ij}$')
ax.set_xlabel('training $C_{ij}$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)
ax.ticklabel_format(style='sci', scilimits=(0,0))

fig.tight_layout()
fig.savefig('meanfield.png')
```

    /home/amayer/.conda/envs/py3/lib/python3.6/site-packages/matplotlib/colors.py:1110: RuntimeWarning: invalid value encountered in less_equal
      mask |= resdat <= 0



![png](notebook_files/meanfield_14_1.png)



```python
invC = np.linalg.inv(cij_flat)
invC4d = reshape_invcov_to_4d(invC, L, q)
Jij_TAP = -2*invC4d/(1+(1-8*invC4d*fi[:, np.newaxis, :, np.newaxis]*fi[np.newaxis, :, np.newaxis, :])**.5)
```

    /home/amayer/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in sqrt
      This is separate from the ipykernel package so we can avoid doing imports until



```python
flatten_ij(Jij_TAP).mean(), flatten_ij(Jij_mf).mean()
```




    (0.19261893913664105, 0.1930600243824052)




```python
plt.hist(flatten_ij(Jij_TAP));
```


![png](notebook_files/meanfield_17_0.png)



```python
plt.plot(flatten_ij(Jij_mf), flatten_ij(Jij_TAP), 'o')
plt.plot([-1, 3], [-1, 3])
```




    [<matplotlib.lines.Line2D at 0x7fac0e32e908>]




![png](notebook_files/meanfield_18_1.png)



```python
L, q = fi.shape
rhoij = np.zeros(fij.shape)
for i in range(L):
    for j in range(L):
        rhoij[i, j] = fij[i, j]/np.outer(fi[i], fi[j]) - 1.0
plt.plot(flatten_ij(rhoij), flatten_ij(zero_sum_gauge(Jij)),
         '.', ms=1, label='fitted')
plt.plot(flatten_ij(rhoij), flatten_ij(zero_sum_gauge(Jij_TAP)),
         '.', ms=1, label='mf')
lim = 1.0
plt.plot([-lim, lim], [-lim, lim], 'k-')
plt.xlabel(r'$\rho_{ij}$')
plt.ylabel('$J_{ij}$')
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fac0e443da0>




![png](notebook_files/meanfield_19_1.png)



```python
arr = np.load('../aafreqpca/data/data.npz')
aa_human = arr['human']
ps = aa_human
blocklength = int(1e7)//len(ps)
sampless = []
for p in ps:
    samples = np.random.choice(np.arange(0, 20, 1),
                               size=(blocklength, L),
                               p=p)
    sampless.append(samples)
sample_composition = np.concatenate(sampless)
```


```python
q = naminoacids
fi = frequencies(sample_composition, num_symbols=q)
fij = pair_frequencies(sample_composition, num_symbols=q, fi=fi)
cij = compute_covariance_matrix(fi, fij)
```


```python
cij_flat = compute_flattened_covariance_matrix(fi, fij)
Jij_comp = compute_Jij_mf(cij_flat)
Jij_comp = zero_sum_gauge(Jij_comp)
hi_comp = fields(Jij_comp, fi)
```


```python
plotting.density_scatter(flatten_ij(Jij), flatten_ij(Jij_comp), s=1)
x = np.linspace(-0.3, 0.7)
plt.plot(x,x)
```




    [<matplotlib.lines.Line2D at 0x7fac0e3ba1d0>]




![png](notebook_files/meanfield_23_1.png)



```python
x0 = prng.randint(q, size=L)
sample_matrix = mcmcsampler(x0, lambda x: energy_potts(x, hi_comp, Jij_comp), jump, 5e6, nsample=10, nburnin=1e4)
```


```python
fi_mf = frequencies(sample_matrix, num_symbols=q)
fij_mf = pair_frequencies(sample_matrix, num_symbols=q, fi=fi_mf)
cij_mf = compute_covariance_matrix(fi_mf, fij_mf)
```


```python
cov = np.cov(aa_human.T)
plt.plot(flatten_ij(cij), flatten_ij(cij_mf), 'o', ms=1)
x = np.linspace(-2e-4, 1e-3)
plt.plot(x,x)
```




    [<matplotlib.lines.Line2D at 0x7fac111d6128>]




![png](notebook_files/meanfield_26_1.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.2), ncols=2)

ax = axes[0]
lims = [0, 0.12]
ax.plot(np.ravel(fi), np.ravel(fi_mf), 'o', ms=2)
ax.set_xlabel('training $f_i$')
ax.set_ylabel('mf $f_i$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)

ax = axes[1]
lims = [-0.3e-2, 0.5e-2]
plotting.density_scatter(flatten_ij(observables_dict['cij']['train']),
                         flatten_ij(cij_mf),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=30, ax=ax)
ax.set_ylabel('mean-field $C_{ij}$')
ax.set_xlabel('training $C_{ij}$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)
ax.ticklabel_format(style='sci', scilimits=(0,0))

fig.tight_layout()
fig.savefig('meanfield.png')
```


![png](notebook_files/meanfield_27_0.png)



```python
fig, axes = plt.subplots(figsize=(7, 3.2), ncols=2)

ax = axes[0]
lims = [0, 0.12]
ax.plot(np.ravel(fi), np.ravel(fi), 'o', ms=2)
ax.set_xlabel('training $f_i$')
ax.set_ylabel('mf $f_i$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)

ax = axes[1]
lims = [-0.3e-2, 0.5e-2]
plotting.density_scatter(flatten_ij(observables_dict['cij']['train']),
                         flatten_ij(cij),
                             norm=colors.LogNorm(vmin=1),
                             s=0.5,
                             bins=30, ax=ax)
ax.set_ylabel('mean-field $C_{ij}$')
ax.set_xlabel('training $C_{ij}$')
ax.plot(lims, lims, 'k')
ax.set_xlim(*lims)
ax.set_ylim(*lims)
ax.ticklabel_format(style='sci', scilimits=(0,0))

fig.tight_layout()
```


![png](notebook_files/meanfield_28_0.png)



```python

```
#### fit_nskew.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

L = 4
nsample = L
output = True
q = naminoacids
pseudocount = 1.0
niter = 200
stepsize = 0.01 
nsteps = 1e6
nburnin = 1e3

prng = np.random

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

arr = np.load('data/Human_ncov_%i.npz'%L)
h = arr['h']
J = arr['J']

def sampler(*args, **kwargs):
    return mcmcsampler(*args, nsteps=nsteps, nsample=nsample, nburnin=nburnin)
h, J, J2 = fit_nskew(matrix, sampler=sampler, h=h, J=J,
                niter=niter, pseudocount=pseudocount,
                epsilon=stepsize, prng=prng, output=output)

@njit
def jump(x):
    return local_jump_jit(x, q)
@njit
def energy(x):
    return energy_nskew(x, h, J, J2)
x0 = prng.randint(q, size=L)
nsteps_generate = int(matrix.shape[0]*nsample)
model_matrix = mcmcsampler(x0, energy, jump, nsteps=nsteps_generate,
                           nsample=nsample, nburnin=nburnin)
np.savetxt('data/model_nskew_matrix_L%i.csv.gz'%L, model_matrix, fmt='%i')
np.savez('data/Human_nskew_%i.npz'%L, h=h, J=J, J2=J2)

```
#### fit_ncov.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

L = 4
nsample = L
output = True
q = naminoacids
pseudocount = 1.0
niter = 50
stepsize = 0.1 
nsteps = 1e7
nburnin = 1e3

prng = np.random

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

def sampler(*args, **kwargs):
    return mcmcsampler(*args, nsteps=nsteps, nsample=nsample, nburnin=nburnin)
h, J = fit_ncov(matrix, sampler=sampler,
                niter=niter, pseudocount=pseudocount,
                epsilon=stepsize, prng=prng, output=output)

@njit
def jump(x):
    return local_jump_jit(x, q)
@njit
def energy(x):
    return energy_ncov(x, h, J)
x0 = prng.randint(q, size=L)
nsteps_generate = int(matrix.shape[0]*nsample)
model_matrix = mcmcsampler(x0, energy, jump, nsteps=nsteps_generate,
                           nsample=nsample, nburnin=nburnin)
np.savetxt('data/model_ncov_matrix_L%i.csv.gz'%L, model_matrix, fmt='%i')
np.savez('data/Human_ncov_%i.npz'%L, h=h, J=J)

```
#### plot_all.py

```python
import numpy as np
import pandas as pd
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
plt.style.use('../peptidome.mplstyle')

from common import labels, model_hierarchy

L = 9

observables = ['fi', 'cij', 'cijk']
observables_dict = {key: dict() for key in observables}
datasets = ['test', 'train']
datasets.extend(model_hierarchy)
for dataset in datasets:
    params = np.load('data/%s_observables_L%i.npz'%(dataset, L))
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]
print(flatten_ijk(observables_dict['cijk']['train']).max())
print(flatten_ijk(observables_dict['cijk']['test']).max())

nrows = len(datasets[1:])
fig, axes = plt.subplots(figsize=(6.0, 1.75*nrows), ncols=3, nrows=nrows)
for j, (observable, label, lims, flattener) in enumerate([('fi', '$f_i$', (0, 0.12), np.ravel),
                                               ('cij', '$C_{ij}$', (-0.0025, 0.0035), flatten_ij),
                                               ('cijk', '$C_{ijk}$', (-3e-4, 6e-4), flatten_ijk)]):
    for i, model_type in enumerate(datasets[1:]):
        ax = axes[i, j]
        if observable in ['cij', 'cijk']:
            plotting.density_scatter(flattener(observables_dict[observable]['test']),
                                     flattener(observables_dict[observable][model_type]),
                                     norm=colors.LogNorm(vmin=1),
                                     s=0.5,
                                     bins=50, ax=ax)
        else:
            ax.plot(flattener(observables_dict[observable]['test']),
                    flattener(observables_dict[observable][model_type]),
                    'o', ms=2 if observable == 'fi' else 1)

        ax.set_xlabel('test %s'%label)
        ax.set_ylabel('%s %s'%(labels[model_type], label))
        ax.plot(lims, lims, 'k')
        ax.set_xlim(*lims)
        ax.set_ylim(*lims)

for ax in axes[:, 1:].flatten():
    ax.ticklabel_format(style='sci', scilimits=(0,0))

label_axes(fig, labelstyle='%s')
fig.tight_layout()
fig.savefig('connected_correlations_allmodels.png')

```
#### fit_nskewfcov.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

L = 4
nsample = L
output = True
q = naminoacids
pseudocount = 1.0
niter = 200
stepsize = 0.01 
nsteps = 1e7
nburnin = 1e3

prng = np.random

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

arr = np.load('data/Human_nskew_%i.npz'%L)
h = arr['h']
J = arr['J']
J2 = arr['J2']

def sampler(*args, **kwargs):
    return mcmcsampler(*args, nsteps=nsteps, nsample=nsample, nburnin=nburnin)
h, J, J2, hi, Jij = fit_nskewfcov(matrix, sampler=sampler, h=h, J=J,
                niter=niter, pseudocount=pseudocount,
                epsilon=stepsize, prng=prng, output=output)

@njit
def jump(x):
    return local_jump_jit(x, q)
@njit
def energy(x):
    return energy_nskewfcov(x, h, J, J2, hi, Jij)
x0 = prng.randint(q, size=L)
nsteps_generate = int(matrix.shape[0]*nsample)
model_matrix = mcmcsampler(x0, energy, jump, nsteps=nsteps_generate,
                           nsample=nsample, nburnin=nburnin)
np.savetxt('data/model_nskewfcov_matrix_L%i.csv.gz'%L, model_matrix, fmt='%i')
np.savez('data/Human_nskewfcov_%i.npz'%L, h=h, J=J, J2=J2, hi=hi, Jij=Jij)

```
#### entropies.py

```python
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *

from fit_all import extra

integration_intervals = 5
L = 9
mcmc_kwargs = dict(nsteps=1e6, nsample=20, nburnin=1e4)

params = np.load('data/Human_9.npz')
hi_human = params['hi']
Jij_human = params['Jij']

def entropy_thermodynamic_integration(hi, Jij,
        integration_intervals=1,
        mcmc_kwargs=dict(), prng=np.random):
    L, q = hi.shape
    
    @njit
    def jump(x):
        return local_jump_jit(x, q)
    @njit
    def energy(x):
        return energy_potts(x, hi, Jij)
    x0 = prng.randint(q, size=L)
    matrix = mcmcsampler(x0, energy, jump, **mcmc_kwargs)
    energy = np.array([energy(x) for x in matrix])
    energy_human = np.array([energy_potts(x, hi_human, Jij_human) for x in matrix])
    energy_mean = np.mean(energy)
    deltaE = np.mean(energy_human-energy)
    
    F = Fpotts_thermodynamic_integration(hi, Jij,
            integration_intervals=integration_intervals, mcmc_kwargs=mcmc_kwargs)
    S = energy_mean - F
    return S, energy_mean, F, deltaE

proteomes = load_proteomes()
if len(sys.argv) < 2:
    print(proteomes.shape[0])
else:
    idx = int(sys.argv[1])-1
    if idx < proteomes.shape[0]:
        row = proteomes.iloc[idx]
        name = row.name
    else:
        proteome, name = extra[idx-proteomes.shape[0]] 
    path = 'data/%s_%g.npz'%(name, L)
    params = np.load(path)
    hi = params['hi']
    Jij = params['Jij']
    S, E, F, Ehuman = entropy_thermodynamic_integration(hi, Jij,
            integration_intervals=integration_intervals, mcmc_kwargs=mcmc_kwargs)
    print(name, S)
    with open('data/entropies.csv', 'a') as f:
        f.write(','.join([str(s) for s in [name, S, E, F, Ehuman]]))

```
#### evaluate.py

```python
import os.path
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

L = 9

for dataset in ['train', 'test', 'flat', 'independent',
        'model', 'model_ncov', 'model_nskew', 'model_nskewdiag', 'model_nskewfcov']:
    print(dataset)
    path = 'data/%s_observables_L%i.npz'%(dataset, L)
    if not os.path.exists(path):
        matrix = load_matrix('data/%s_matrix_L%i.csv.gz'%(dataset, L))
        fi = frequencies(matrix, num_symbols=naminoacids)
        fij = pair_frequencies(matrix, num_symbols=naminoacids, fi=fi)
        cij = compute_covariance_matrix(fi, fij)
        fijk = triplet_frequencies(matrix, num_symbols=naminoacids)
        cijk = compute_cijk(fijk, fij, fi)
        fold_ijk = compute_fold_ijk(fijk, fi)
        np.savez_compressed(path,
                 fi=fi, fij=fij, cij=cij,
                 cijk=cijk, fijk=fijk, fold_ijk=fold_ijk)

```
#### fit_independent.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *

from numba import njit

L = 9
pseudocount = 1.0

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

flat = np.random.choice(np.arange(0, len(aminoacids), 1), size=matrix.shape)
np.savetxt('data/flat_matrix_L%i.csv.gz'%L, flat, fmt='%i')

fi = frequencies(matrix, num_symbols=len(aminoacids), pseudocount=pseudocount)
f = fi.mean(axis=0)
independent = np.random.choice(np.arange(0, 20, 1), size=matrix.shape, p=f)
np.savetxt('data/independent_matrix_L%i.csv.gz'%L, independent, fmt='%i')

```
#### fit_all.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

output = True
aas_arr = np.array(list(aminoacids))
L = 9
q = naminoacids
pseudocount = 1e-3
niter = 30
stepsize = 0.1
mcmc_kwargs = dict(nsteps=1e6, nsample=20, nburnin=1e4)
extra = [(datadir+'human-viruses-swissprot.fasta', 'viruses')]

if __name__ == "__main__":
    proteomes = load_proteomes()
    if len(sys.argv) < 2:
        print(proteomes.shape[0])
    else:
        idx = int(sys.argv[1])-1
        if idx < proteomes.shape[0]:
            row = proteomes.iloc[idx]
            name = row.name
            proteome = proteome_path(name)
        else:
            proteome, name = extra[idx-proteomes.shape[0]] 
        print(name)

        matrix = kmers_to_matrix(to_kmers(fasta_iter(proteome, returnheader=False), k=L))
        fi = frequencies(matrix, num_symbols=q, pseudocount=pseudocount)
        fij = pair_frequencies(matrix, num_symbols=q, fi=fi, pseudocount=pseudocount)

        prng = np.random
        def sampler(*args, **kwargs):
            mcmc_kwargs.update(kwargs)
            return mcmcsampler(*args, **mcmc_kwargs)
        hi, Jij = fit_full_potts(fi, fij, sampler=sampler, niter=niter,
                                 epsilon=stepsize, prng=prng, output=output)

        np.savez('data/%s_%g.npz'%(name, L), hi=hi, Jij=Jij)

```
#### fit_nskewdiag.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

L = 9
nsample = L
output = True
q = naminoacids
pseudocount = 1.0
niter = 200
stepsize = 0.01 
nsteps = 1e6
nburnin = 1e3

prng = np.random

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

arr = np.load('data/Human_ncov_%i.npz'%L)
h = arr['h']
J = arr['J']

def sampler(*args, **kwargs):
    return mcmcsampler(*args, nsteps=nsteps, nsample=nsample, nburnin=nburnin)
h, J, J2 = fit_nskewdiag(matrix, sampler=sampler, h=h, J=J,
                niter=niter, pseudocount=pseudocount,
                epsilon=stepsize, prng=prng, output=output)

@njit
def jump(x):
    return local_jump_jit(x, q)
@njit
def energy(x):
    return energy_nskewdiag(x, h, J, J2)
x0 = prng.randint(q, size=L)
nsteps_generate = int(matrix.shape[0]*nsample)
model_matrix = mcmcsampler(x0, energy, jump, nsteps=nsteps_generate,
                           nsample=nsample, nburnin=nburnin)

np.savetxt('data/model_nskewdiag_matrix_L%i.csv.gz'%L, model_matrix, fmt='%i')
np.savez('data/Human_nskewdiag_%i.npz'%L, h=h, J=J, J2=J2)

```
#### generate_reference_matrices.py

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

L = 4
seed = 12345

prng = np.random.RandomState(seed=seed)
seqs = np.array(pd.read_csv('../pfam/data/human_nozf.csv')['Sequence'])
train, test = train_test_split(seqs, test_size=0.5, random_state=prng)

for label, data in [('train', train), ('test', test)]:
    matrix = kmers_to_matrix(to_kmers(data, k=L))
    np.savetxt('data/%s_matrix_L%i.csv.gz'%(label,L), matrix, fmt='%i')

```
#### fit.py

```python
import itertools, json
import numpy as np
import pandas as pd

import sys
sys.path.append('..')
from lib import *
from lib.maxent import *

from numba import njit

L = 9
nsample = L
output = True
q = naminoacids
pseudocount = 1.0
niter = 50
stepsize = 0.1 
nsteps = 1e7
nburnin = 1e3

prng = np.random

matrix = load_matrix('data/train_matrix_L%i.csv.gz'%L)

def sampler(*args, **kwargs):
    return mcmcsampler(*args, nsteps=nsteps, nsample=nsample, nburnin=nburnin)
hi, Jij = fit_full_potts(matrix, sampler=sampler,
                         niter=niter, pseudocount=pseudocount,
                         epsilon=stepsize, prng=prng, output=output)

@njit
def jump(x):
    return local_jump_jit(x, q)
@njit
def energy(x):
    return energy_potts(x, hi, Jij)
x0 = prng.randint(q, size=L)
nsteps_generate = int(matrix.shape[0]*nsample)
model_matrix = mcmcsampler(x0, energy, jump, nsteps=nsteps_generate,
                           nsample=nsample, nburnin=nburnin)
np.savetxt('data/model_matrix_L%i.csv.gz'%L, model_matrix, fmt='%i')
np.savez('data/Human_model_%i.npz'%L, hi=hi, Jij=Jij)

```
#### plot.py

```python
import numpy as np
import pandas as pd
from matplotlib import colors

import sys
sys.path.append('..')
from lib import *
plt.style.use('../peptidome.mplstyle')


observables = ['fi', 'cij', 'cijk']
observables_dict = {key: dict() for key in observables}
datasets = ['train', 'test', 'model', 'model_ncov', 'model_nskew', 'model_nskewdiag']
for dataset in datasets:
    params = np.load('data/%s_observables.npz'%dataset)
    for observable in observables:
        observables_dict[observable][dataset] = params[observable]

for model_type in datasets[2:]:
    fig, axes = plt.subplots(figsize=(6, 3.5), ncols=3, nrows=2)

    for j, (observable, label, lims, flattener) in enumerate([('fi', '$f_i$', (0, 0.12), np.ravel),
                                                   ('cij', '$C_{ij}$', (-0.0025, 0.0035), flatten_ij),
                                                   ('cijk', '$C_{ijk}$', (-4e-4, 7e-4), flatten_ijk)]):
        for i, dataset in enumerate([model_type, 'test']):
            ax = axes[i, j]
            if observable in ['cij', 'cijk']:
                plotting.density_scatter(flattener(observables_dict[observable]['train']),
                                         flattener(observables_dict[observable][dataset]),
                                         norm=colors.LogNorm(vmin=1),
                                         s=0.5,
                                         bins=50, ax=ax)
            else:
                ax.plot(flattener(observables_dict[observable]['train']),
                        flattener(observables_dict[observable][dataset]),
                        'o', ms=2 if observable == 'fi' else 1)

            ax.set_xlabel('train %s'%label)
            ax.set_ylabel('%s %s'%(dataset, label))
            ax.plot(lims, lims, 'k')
            ax.set_xlim(*lims)
            ax.set_ylim(*lims)

    for ax in axes[:, 1:].flatten():
        ax.ticklabel_format(style='sci', scilimits=(0,0))

    label_axes(fig, labelstyle='%s')
    fig.tight_layout()

    fig.savefig('main.png' if model_type == 'model' else model_type+'.png')
    if model_type == 'model':
        fig.savefig('../../paper/images/maxent_freqs.pdf')
    elif model_type == 'model_global':
        fig.savefig('../../paper/images/maxent_freqs_global.pdf')
    plt.show()

```
#### common.py

```python
model_hierarchy = ['model_ncov', 'model', 'model_nskewdiag', 'model_nskew', 'model_nskewfcov']

labels = {'test': 'test set',
          'train': 'training set',
          'model': '2-point',
          'model_ncov':'2nd moment',
          'independent' : '1-point',
          'model_nskew': '3rd moment',
          'model_nskewdiag': '3rd moment diag',
          'model_nskewfcov' : '3rd moment, 2-point'}

def set_aminoacidslabel(ax, aminoacidorder):
    naminoacids = len(aminoacidorder)
    ax.set_xticks(range(naminoacids))
    ax.set_yticks(range(naminoacids))
    ax.set_xticklabels(list(aminoacidorder))
    ax.set_yticklabels(list(aminoacidorder))

```
