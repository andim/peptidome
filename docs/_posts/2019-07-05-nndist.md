---
layout: post
title: Distributions of nearest neighbor distances
---

Is the finiteness of the self proteome important?

{% include post-image-gallery.html filter="nndist/" %}

### Code 
#### nn_analysis.ipynb



```python
import numpy as np
import pandas as pd
from scipy.stats import entropy
import scipy.stats
from scipy.stats import poisson
import seaborn as sns
import sklearn.neighbors
import matplotlib.pyplot as plt
%matplotlib inline

import Levenshtein

import sys
sys.path.append('..')

from lib import *
```


```python
df_t = load_iedb_tcellepitopes(human_only=True)
```


```python
S = 20
k = 9

Ntot = S**k
Nn = k*(S-1)
```


```python
counter9 = count_kmers_proteome(human, k, clean=True)
```


```python
human9 = set(counter9)
Nhuman = len(human9)
```


```python
df = Counter(human, 1).to_df(norm=True, clean=True)
paa = np.asarray(df['freq'])
paa
pi = paa

psigmas = []
nsigmas = []
Nsample = 1000000
for i in range(Nsample):
    sigma = np.random.randint(0, S, k)
    psigma = np.prod(pi[sigma])
    nsigma = np.prod(pi[sigma])*(np.sum(1/pi[sigma]) - k)
    psigmas.append(psigma)
    nsigmas.append(nsigma)
nsigmas = np.asarray(nsigmas)
psigmas = np.asarray(psigmas)

p0 = 1/Ntot
pp = Ntot*np.mean(psigmas**2)
print(pp, p0)
n0 = Nn/Ntot
n = Ntot*np.mean(psigmas*nsigmas)
print(n0, n)
```

    9.321741899587087e-12 1.953125e-12
    3.33984375e-10 1.3278175401433792e-09



```python
tripletparams = calc_tripletmodelparams(human)
```


```python
ptri = lambda seq: 10**loglikelihood_triplet(map_numbertoaa(seq), **tripletparams)

```


```python
psigmas = []
Nsample = 1000000
for i in range(Nsample):
    sigma = np.random.randint(0, S, k)
    psigma = ptri(sigma)
    psigmas.append(psigma)
    nsigma = np.prod(pi[sigma])*(np.sum(1/pi[sigma]) - k)
psigmas = np.asarray(psigmas)

pptri = Ntot*np.mean(psigmas**2)
pptri
```




    1.597433624031817e-11




```python
def neighbors(sigma, S):
    for i in range(len(sigma)):
        for s in range(S):
            if not sigma[i] == s:
                yield np.asarray(list(sigma[:i]) + [s] + list(sigma[i+1:]))
```


```python
psigmas = []
nsigmas = []
Nsample = 10000
for i in range(Nsample):
    sigma = np.random.randint(0, S, k)
    psigma = ptri(sigma)
    nsigma = np.sum(np.fromiter((ptri(sigmap) for sigmap in neighbors(sigma, S)), np.float))
    psigmas.append(psigma)
    nsigmas.append(nsigma)
psigmas = np.asarray(psigmas)
nsigmas = np.asarray(nsigmas)

pptri = Ntot*np.mean(psigmas**2)
ntri = Ntot*np.mean(psigmas*nsigmas)
print(pptri, ntri/Nn)
```

    1.5902799045314093e-11 1.1849812843728578e-11



```python
# no human epitopes
mask = ~df_t['Epitope', 'Parent Species'].str.contains('Homo sapiens', na=False)
# no epitopes of unknown provenance
mask &= ~df_t['Epitope', 'Parent Species'].isna()
# only epitopes of length 9
mask &= df_t['Epitope', 'Description'].apply(len)==9
# only infectious disease epitopes
#mask &= df_t['1st in vivo Process', 'Process Type'] == 'Occurrence of infectious disease'
d = df_t[mask]
```


```python
d['1st in vivo Process', 'Process Type'].value_counts()
```




    Administration in vivo                                                             15122
    Occurrence of infectious disease                                                    5965
    Exposure with existing immune reactivity without evidence for disease               3684
    Environmental exposure to endemic/ubiquitous agent without evidence for disease     2664
    No immunization                                                                      994
    Documented exposure without evidence for disease                                     244
    Exposure without evidence for disease                                                147
    Occurrence of autoimmune disease                                                      94
    Occurrence of cancer                                                                  89
    Occurrence of allergy                                                                 55
    Occurrence of disease                                                                 24
    Unknown                                                                               17
    Transplant/transfusion                                                                16
    Name: (1st in vivo Process, Process Type), dtype: int64




```python
d0 = d[d['Epitope', 'Description'].apply(lambda x: x in human9)]
#d0.head()
```


```python
d0['Epitope', 'Parent Species'].unique()
```




    array(['Mycobacterium tuberculosis', 'Vaccinia virus',
           'Human betaherpesvirus 5', 'Chlamydia trachomatis', 'Mus musculus',
           'Cavia porcellus', 'Human endogenous retrovirus K',
           'Leishmania major', 'Dengue virus', 'Human betaherpesvirus 6B'],
          dtype=object)




```python
d0.shape[0], Nhuman*d.shape[0]*p0, Nhuman*d.shape[0]*pp
```




    (113, 0.5915592975117188, 2.8233539019291096)




```python
def dist1(x):
    for i in range(len(x)):
        for aa in aminoacids:
            if aa == x[i]:
                continue
            if x[:i]+aa+x[i+1:] in human9:
                return True
    return False
```


```python
d1 = d[d['Epitope', 'Description'].apply(dist1)]
d1['Epitope', 'Parent Species'].unique()
```




    array(['Hepatitis B virus', 'Mycobacterium tuberculosis',
           'Influenza A virus', 'Dengue virus', 'Hepacivirus C',
           'Borreliella burgdorferi', 'Vaccinia virus',
           'Streptococcus pyogenes', 'Plasmodium falciparum',
           'Plasmodium vivax', 'Leishmania donovani', 'Trypanosoma cruzi',
           'Human gammaherpesvirus 4', 'Chlamydia trachomatis',
           'Human betaherpesvirus 5', 'Measles morbillivirus',
           'Severe acute respiratory syndrome-related coronavirus',
           'Triticum aestivum', 'Human metapneumovirus',
           'Primate T-lymphotropic virus 1', 'Mus musculus',
           'Human alphaherpesvirus 3', 'Mycobacterium kansasii',
           'Toxoplasma gondii', 'Human alphaherpesvirus 1',
           'Human gammaherpesvirus 8', 'Alphapapillomavirus 9',
           'Mycobacterium leprae', 'Human endogenous retrovirus K',
           'Human mastadenovirus B', 'Leishmania major',
           'Human immunodeficiency virus 1', 'Human mastadenovirus C',
           'Bacteroides stercoris', 'Human betaherpesvirus 6B'], dtype=object)




```python
Ncomp = d.shape[0]*len(human9)
Ncomp*n0, Ncomp*n
```




    (101.15663987450391, 402.1671993707294)




```python
# no human epitopes
#mask = ~df_t['Epitope', 'Parent Species'].str.contains('Homo sapiens', na=False)
# no epitopes of unknown provenance
mask = ~df_t['Epitope', 'Parent Species'].isna()
# only epitopes of length 9
mask &= df_t['Epitope', 'Description'].apply(len)==9

mask_noh = mask[:]
mask_noh &= ~df_t['Epitope', 'Parent Species'].str.contains('Homo sapiens', na=False)
mask1 = mask_noh[:]
mask1 &= ~(df_t['Assay', 'Qualitative Measure'] == 'Negative')
mask2 = mask_noh[:]
mask2 &= df_t['Assay', 'Qualitative Measure'] == 'Negative'

mask &= ~(df_t['Assay', 'Qualitative Measure'] == 'Negative')
mask3 = mask[:]
mask3 &= df_t['1st in vivo Process', 'Process Type'] == 'Occurrence of infectious disease'
mask4 = mask[:]
mask4 &= df_t['1st in vivo Process', 'Process Type'] == 'Occurrence of allergy'
mask5 = mask[:]
mask5 &= df_t['1st in vivo Process', 'Process Type'] == 'Occurrence of autoimmune disease'
```


```python
Nhuman_tot = sum(val for val in counter9.values())
Nhuman_tot, Nhuman
```




    (11349410, 10401757)




```python
cases = [('positive', mask1),
         ('negative', mask2),
         ('infectious', mask3),
         ('allergy', mask4),
         ('autoimmune', mask5),
        ]
fig, axes = plt.subplots(figsize=(3.42, 2.0*len(cases)), nrows=len(cases), sharex=False)
print(1/Ntot, pp, n/Nn)
for i, (name, mask) in enumerate(cases):
    print(name)
    ax = axes[i]
    d = df_t[mask]
    count0 = d[d['Epitope', 'Description'].apply(lambda x: x in human9)].shape[0]
    count0_tot = np.sum(d['Epitope', 'Description'].apply(lambda x: counter9[x] if x in counter9 else 0))
    #print(count0, count0_tot)
    print(count0/(d.shape[0]*Nhuman), count0_tot/(d.shape[0]*Nhuman_tot))
    count1 = d[d['Epitope', 'Description'].apply(dist1)].shape[0] 
    print(count1/(d.shape[0]*Nhuman*Nn))
    ax.axvline(count1, c='k')
    ax.set_title(name + ' N = %g' % d.shape[0])
    x = np.arange(0, count1*1.5)
    Ncomp = d.shape[0]*Nhuman
    mu = Ncomp*n0
    ax.plot(x, poisson.pmf(x, mu), '-', ms=8, label='flat')
    mu = Ncomp*n
    ax.plot(x, poisson.pmf(x, mu), '-', ms=8, label='independent')
    mu = Ncomp*ntri
    ax.plot(x, poisson.pmf(x, mu), '-', ms=8, label='tri')
    #ax.legend()
    ax.set_ylim(0.0)
    ax.set_xlim(min(x), max(x))
    ax.set_yticks([])
ax.set_xlabel('Peptides at distance 1 to self')
fig.tight_layout()
```

    1.953125e-12 9.321741899587087e-12 7.765014854639644e-12
    positive
    2.1276913165079495e-10 2.80804802939264e-10
    1.338827986293891e-11
    negative
    4.747003249449207e-10 7.663055545740884e-10
    1.1545741555677896e-11
    infectious
    1.4783293331297674e-09 2.286379904374286e-09
    1.8371051654390385e-11
    allergy
    2.9580801367721598e-08 3.795520919318544e-08
    9.514292837571274e-11
    autoimmune
    8.438745279069467e-08 1.09811549348935e-07
    9.786587413730722e-11



![png](notebook_files/nn_analysis_21_1.png)


# Analysis of flu epitopes


```python
fluepis = df_t[df_t['Epitope', 'Parent Species'] == 'Influenza A virus']#['Epitope', 'Description'].unique()
fluepis.shape
```




    (3024, 141)




```python
fluepis['Epitope', 'Description'].unique().shape
```




    (1266,)




```python
plt.hist([len(s) for s in fluepis['Epitope', 'Description'].unique()], bins=np.arange(8, 21))
```




    (array([ 13., 114.,  58.,  27.,  14.,  65.,  31.,  86.,  53., 314., 234.,
            196.]),
     array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]),
     <a list of 12 Patch objects>)




![png](notebook_files/nn_analysis_25_1.png)



```python
nchunks = 100
btdist = BallTreeDist(human9, nchunks=nchunks)
```


```python
fludists = [btdist.mindist(e) for e in fluepis[fluepis['Epitope', 'Description'].apply(len)==9]]
```


    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    <ipython-input-19-0eb73821c06c> in <module>
    ----> 1 fludists = [btdist.mindist(e) for e in fluepis[fluepis['Epitope', 'Description'].apply(len)==9]]
    

    <ipython-input-19-0eb73821c06c> in <listcomp>(.0)
    ----> 1 fludists = [btdist.mindist(e) for e in fluepis[fluepis['Epitope', 'Description'].apply(len)==9]]
    

    ~/repos/peptidome/code/lib/neighbors.py in mindist(self, sequence)
         17 
         18     def mindist(self, sequence):
    ---> 19         sequence_number= map_aatonumber(sequence).reshape(1, -1)
         20         d = min(bt.query(sequence_number)[0] for bt in self.bts)
         21         return int(d*len(sequence))


    ~/repos/peptidome/code/lib/main.py in map_aatonumber(seq)
        357     """
        358     seq = np.array(list(seq))
    --> 359     return np.vectorize(_aatonumber.__getitem__)(seq)
        360 
        361 def map_numbertoaa(seq):


    ~/.conda/envs/py3/lib/python3.6/site-packages/numpy/lib/function_base.py in __call__(self, *args, **kwargs)
       2089             vargs.extend([kwargs[_n] for _n in names])
       2090 
    -> 2091         return self._vectorize_call(func=func, args=vargs)
       2092 
       2093     def _get_ufunc_and_otypes(self, func, args):


    ~/.conda/envs/py3/lib/python3.6/site-packages/numpy/lib/function_base.py in _vectorize_call(self, func, args)
       2159             res = func()
       2160         else:
    -> 2161             ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)
       2162 
       2163             # Convert args to object arrays first


    ~/.conda/envs/py3/lib/python3.6/site-packages/numpy/lib/function_base.py in _get_ufunc_and_otypes(self, func, args)
       2119 
       2120             inputs = [arg.flat[0] for arg in args]
    -> 2121             outputs = func(*inputs)
       2122 
       2123             # Performance note: profiling indicates that -- for simple


    KeyError: 'Reference'



```python
proteomes = load_proteomes()

```


```python
df_flua = counter_to_df(count_kmers_proteome(datadir + proteomes.loc['InfluenzaA']['path'], 9), norm=True)
df_flua.head()
```


```python
distss = []
for i in range(3):
    peptides = np.random.choice(df_flua['seq'], size=len(fludists), replace=False, p=df_flua['freq'])
    dists = [btdist.mindist(e) for e in peptides]
    distss.append(dists)
```


```python
counts = np.bincount(fludists)
print(counts)
plt.plot(counts, 'o')
for d in [distss[0], distss[1], distss[2]]:
    counts = np.bincount(d)
    print(counts)
    plt.plot(counts, 'kx')
```


```python
def dist2(x):
    for i in range(len(x)):
        for j in range(i+1, len(x)):
            for aai in aminoacids:
                si = x[:i]+aai+x[i+1:]
                for aaj in aminoacids:
                    if (aai == x[i]) and (aaj == x[j]):
                        continue
                    if si[:j]+aaj+si[j+1:] in human9:
                        return True
    return False
```


```python
def dists_direct(df, ref):
    d0 = df[df['seq'].apply(lambda x: x in ref)].shape[0]/df['seq'].shape[0]
    d1 = df[df['seq'].apply(dist1)].shape[0]/df['seq'].shape[0]
    d2 = df[df['seq'].apply(dist2)].shape[0]/df['seq'].shape[0]
    return d0, d1, d2
```


```python
distsallflu = dists_direct(df_flua, human9)
```


```python
counts = np.bincount(fludists)
plt.plot(counts/np.sum(counts), 'o')
plt.plot(distsallflu, 'x')

N = sum(counter9.values())
print('%e'%N)
k = 9
K = 20**k

dists = np.arange(6)
Nc = lambda d: 19**dists * falling_factorial(k, dists+1)
cumulative = [0]
cumulative.extend(1-np.exp(-Nc(dists)*N/K))
plt.plot(dists, np.diff(cumulative), '+')
plt.ylim(0.0)
```


```python
hivepis = d[d['Epitope', 'Parent Species'] == 'Human immunodeficiency virus 1']['Epitope', 'Description'].unique()
hivepis.shape
```


```python
hivdists = [mindist_sklearn_chunked(e, bts) for e in hivepis]
```


```python
counts = np.bincount(hivdists)
print(counts)
plt.plot(counts, 'x')
```


```python
df_hiv1 = counter_to_df(count_kmers_proteome(datadir + proteomes.loc['HIV']['path'], 9), norm=True)
df_hiv1.head()
```


```python
distss_hiv = []
for i in range(3):
    peptides = np.random.choice(df_hiv1['seq'], size=len(hivdists), replace=False, p=df_hiv1['freq'])
    dists = [mindist_sklearn_chunked(e, bts) for e in peptides]
    distss_hiv.append(dists)
```


```python
counts = np.bincount(hivdists)
print(counts)
plt.plot(counts, 'o')
for d in distss_hiv:
    counts = np.bincount(d)
    print(counts)
    plt.plot(counts, 'kx')
```


```python
distsallhiv = dists_direct(df_hiv1, human9)
```


```python
counts = np.bincount(hivdists)
plt.plot(counts/np.sum(counts), 'o')
plt.plot(distsallhiv, 'x')
N = sum(counter9.values())
print('%e'%N)
k = 9
K = 20**k

dists = np.arange(6)
Nc = lambda d: 19**dists * falling_factorial(k, dists+1)
cumulative = [0]
cumulative.extend(1-np.exp(-Nc(dists)*N/K))
plt.plot(dists, np.diff(cumulative), '+')
plt.ylim(0.0)
```


```python

```
#### benchmark.ipynb


# Benchmarking hamming distance calculation


```python
import numpy as np
import pandas as pd
from scipy.stats import entropy
import scipy.stats
import seaborn as sns
import sklearn.neighbors
import matplotlib.pyplot as plt
%matplotlib inline

import Levenshtein

import sys
sys.path.append('..')

from lib import *
```


```python
k = 9
counter9 = count_kmers_proteome(human, k, clean=True)
human9 = set(counter9)
```


```python
humansample = random.sample(human9, 100000)
points = np.asarray([map_aatonumber(h) for h in humansample])

```


```python
def mindist(x, sample):
    return min(Levenshtein.hamming(s, x) for s in sample)
```


```python
mindist('AAACCCAAA', humansample)
```




    3




```python
%timeit -t mindist('AAACCCAAA', humansample)
```

    39.8 ms ± 1.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)



```python
bt = sklearn.neighbors.BallTree(points, metric='hamming')
```


```python
def mindist_sklearn(x, tree):
    d, i = tree.query(map_aatonumber(x).reshape(1, -1))
    return int(d*len(x))
```


```python
mindist_sklearn('AAACCCAAA', bt)
```




    3




```python
%timeit -t mindist_sklearn('AAACCCAAA', bt)
```

    6.2 ms ± 224 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


## on all 9mers 


```python
mindist('AAACCCAAA', human9)
```




    2




```python
%timeit -t mindist('AAACCCAAA', human9)
```

    4.42 s ± 182 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)



```python
nchunks = 100
```


```python
human9_number = np.asarray([map_aatonumber(h) for h in human9])
```


```python
pointss = np.array_split(human9_number, nchunks)
```


```python
bts = [sklearn.neighbors.BallTree(points, metric='hamming') for points in pointss]
```


```python
def mindist_sklearn_chunked(x, trees):
    d = min(bt.query(map_aatonumber(x).reshape(1, -1))[0] for bt in trees)
    return int(d*len(x))
```


```python
mindist_sklearn_chunked('AAACCCAAA', bts)
```




    2




```python
%timeit -t mindist_sklearn_chunked('AAACCCAAA', bts)
```

    711 ms ± 44.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)



```python
btdist = BallTreeDist(human9, nchunks=nchunks)
```


```python
btdist.mindist('AAACCCAAA')
```




    2




```python
%timeit -t btdist.mindist('AAACCCAAA')
```

    695 ms ± 45.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)



```python

```
#### math.ipynb


# How does neighbor density relate to likelihoods ?


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use('../peptidome.mplstyle')
#plt.style.use('talk')

import sys
sys.path.append('..')

from lib import *
```

## check summation formula


```python
def neighbors(sigma, S):
    for i in range(len(sigma)):
        for s in range(S):
            if not sigma[i] == s:
                yield np.asarray(list(sigma[:i]) + [s] + list(sigma[i+1:]))

S = 2
k = 2
sigma_lognormal = 1.0
pi = np.random.lognormal(sigma=sigma_lognormal, size=S)
pi /= pi.sum()
sigma = np.random.randint(0, S, k)
nsigma = np.prod(pi[sigma])*(np.sum(1/pi[sigma]) - k)
nsigma_sum = np.sum(np.fromiter((np.prod(pi[sigmap]) for sigmap in neighbors(sigma, S)), np.float))
print(nsigma, nsigma_sum)
```

    0.43379441914720135 0.4337944191472014


## correlation between probabilities


```python
S = 20
k = 10
sigma = 0.3
pi = np.random.lognormal(sigma=sigma, size=S)
pi /= pi.sum()
psigmas = []
psigmaps = []
Nsample = 10000
for i in range(Nsample):
    sigma = np.random.randint(0, S, k)
    psigma = np.prod(pi[sigma])
    i = np.random.randint(0, k)
    sigmai = np.random.choice([s for s in range(0, S) if s != sigma[i]])
    sigmap = np.asarray(list(sigma[:i]) + [sigmai] + list(sigma[i+1:]))
    psigmap = np.prod(pi[sigmap])
    psigmas.append(psigma)
    psigmaps.append(psigmap)
psigmaps = np.asarray(psigmaps)
psigmas = np.asarray(psigmas)
```


```python
fig, ax = plt.subplots(figsize=(3.42, 3.42))
trans = np.log10
ax.plot(trans(psigmas), trans(psigmaps), '.', label='data', ms=1)
ax.plot(trans(psigmas), trans(psigmas), '-', label='linear')
#psigmas_theory = np.linspace(0.5*S**(-k), 2*S**(-k))
#ax.plot(np.log10(psigmas_theory), np.log10(psigmas_theory)+np.log10(k*(S-1)-S*(psigmas_theory*S**k-1)),
#        '-', label='theory')
#ax.plot([-k*np.log10(S)], [-k*np.log10(S) + np.log10(k*(S-1))], 'o')
ax.set_xlabel('$\log_{10} p(\sigma)$')
ax.set_ylabel("$\log_{10} p(\sigma')$")
```




    Text(0, 0.5, "$\\log_{10} p(\\sigma')$")




![png](notebook_files/math_6_1.png)



```python
rhop = np.corrcoef(psigmaps, psigmas)[0, 1]
rhop, np.corrcoef(np.log(psigmaps), np.log(psigmas))[0, 1]
```




    (0.8491838235254919, 0.8955826296953464)



## generate samples from P_uniform(psigma, nsigma)


```python
df = Counter(human, 1).to_df(norm=True, clean=True)
paa = np.asarray(df['freq'])
paa
```




    array([0.02132759, 0.06577845, 0.07012693, 0.06314819, 0.09970845,
           0.08332527, 0.01217275, 0.05643573, 0.05963846, 0.07101057,
           0.02662735, 0.02305051, 0.05347764, 0.0473157 , 0.05724314,
           0.04330068, 0.04767035, 0.02626757, 0.0365297 , 0.03584496])




```python
S = 20
k = 9

Nn = k*(S-1)
N = float(S**k)

#sigma_lognormal = 0.4
#pi = np.random.lognormal(sigma=sigma_lognormal, size=S)
pi = paa
#pi = np.random.uniform(size=S)
pi /= pi.sum()


psigmas = []
nsigmas = []
Nsample = 100000
for i in range(Nsample):
    sigma = np.random.randint(0, S, k)
    psigma = np.prod(pi[sigma])
    nsigma = np.prod(pi[sigma])*np.sum((1-pi[sigma])/pi[sigma])
    psigmas.append(psigma)
    nsigmas.append(nsigma)
nsigmas = np.asarray(nsigmas)
psigmas = np.asarray(psigmas)
rho = np.corrcoef(psigmas, nsigmas)[1, 0]
print(r'$\rho_{p(\sigma), n(\sigma)}$:', rho)
```

    $\rho_{p(\sigma), n(\sigma)}$: 0.9906201713914548



```python
np.var(np.log(psigmas))/k, np.var(np.log(pi))
```




    (0.2650286172753722, 0.2637393500109682)




```python
sigmasq = np.var(np.log(pi))*k
np.var(psigmas), (np.exp(sigmasq)-1)/N**2
```




    (1.5312729094932873e-23, 3.714200465543339e-23)




```python
fig, ax = plt.subplots(figsize=(3.42, 3.42))
ax.plot(np.log10(psigmas), np.log10(nsigmas), '.', label='data', ms=1)
ax.plot(np.log10(psigmas), np.log10(psigmas)+np.log10(k*(S-1)), '-', label='linear')
#factor = 20
#psigmas_theory = np.linspace(S**(-k)/factor, factor*S**(-k))
psigmas_theory = np.linspace(min(psigmas), max(psigmas))
ax.plot(np.log10(psigmas_theory),
        #np.log10(psigmas_theory) + np.log10(Nn - S*(psigmas_theory*float(S**k) - 1)),
        np.log10(psigmas_theory) + np.log10(Nn - S*np.log(psigmas_theory*N)),
        'k-', label='theory', lw=2)
#ax.axvline(-k*np.log10(S), c='k')
ax.set_xlabel('$\log_{10} p(\sigma)$\n Log-Likelihood')
ax.set_ylabel("Log-Neighborlikelihood\n $\log_{10} n(\sigma) = \log_{10} \sum_{\sigma' \sim \sigma} p(\sigma')$")
ax.legend()
fig.tight_layout()
fig.savefig('main.png')
fig.savefig('../../paper/images/nnproblikelihood.pdf')

slope, intercept = np.polyfit(np.log10(psigmas), np.log10(nsigmas), 1)
rho1 = 1 - S/(k*(S-1))
print('slope, prediction', slope, rho1)
nsigmavar_pred = np.var(psigmas)* Nn**2 * rho1**2
nsigmavar_pred2 = Nn*np.var(psigmas)*(1+(Nn-1)*rhop)
print('nsigmavar (sampled, upper, pred):',
      np.var(nsigmas),
      np.var(psigmas)*Nn**2,
      nsigmavar_pred,
      nsigmavar_pred2)
nsigmabar = np.sum(nsigmas*psigmas/np.sum(psigmas))
nsigmabar_upper = Nn*np.mean(psigmas**2)*N
nsigmabar_pred = Nn/N + N*Nn*rho1*np.var(psigmas)
nsigmabar_pred2 = Nn/N + N*(Nn-1.5*S)*np.var(psigmas)
nsigmabar_pred3 = Nn/N + N*(np.var(psigmas) * nsigmavar_pred2)**.5
print('nsigmabar (lower, sampled, upper):', Nn/N, nsigmabar, nsigmabar_upper)
print('prediction (-S, -3/2S, rhop)', nsigmabar_pred, nsigmabar_pred2, nsigmabar_pred3)
```

    slope, prediction 0.8653734425950762 0.8830409356725146
    nsigmavar (sampled, upper, pred): 2.7861268219362906e-19 4.477595114649322e-19 3.4914553609356446e-19 3.806250426060438e-19
    nsigmabar (lower, sampled, upper): 3.33984375e-10 1.376673489089933e-09 1.6786749617905207e-09
    prediction (-S, -3/2S, rhop) 1.5178420867874503e-09 1.4394409138213942e-09 1.5700597083794556e-09



![png](notebook_files/math_13_1.png)



```python
bins = np.linspace(0.0, np.percentile(nsigmas, 99), 100)
fig, ax = plt.subplots()
histkwargs = dict(bins=bins, histtype='step')
ax.hist(psigmas*k*(S-1), label='$P(\sigma)$ rescaled', **histkwargs)
ax.hist(nsigmas, label='$n(\sigma)$', **histkwargs)
ax.legend()
ax.axvline(1/S**k * k*(S-1), c='k')
ax.set_xlim(min(bins), max(bins))
ax.set_yticks([])
ax.set_ylabel('Density')
ax.set_xlabel('Probability');
```


![png](notebook_files/math_14_0.png)


## TODO

- What about longer distances? (Second, third neighbours etc.?) Do things generalize?
- Test with rough Mount Fuji?


```python

```
