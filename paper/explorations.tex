\documentclass[superscriptaddress,onecolumn,pre]{revtex4}
\bibliographystyle{apsrev}

\usepackage{ifthen}
\newboolean{pnas}
\setboolean{pnas}{false}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\graphicspath{{images/}}
\usepackage{color}
\usepackage[pdfstartview=FitH,
            breaklinks=true,
            bookmarksopen=false,
            bookmarksnumbered=true,
            colorlinks=true,
            linkcolor=black,
            citecolor=black,
            urlcolor=black,
            pdftitle={Peptidome},
            pdfauthor={Andreas Mayer},
            pdfsubject={}
            ]{hyperref}
\newcommand{\B}{\boldsymbol}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}

\def\(({\left(}
\def\)){\right)}                       
\def\[[{\left[}
\def\]]{\right]}

\newcommand{\AM}[1]{{\color{blue}#1}}

\begin{document}

\title{Peptidome: explorations}
\date{\today}

\begin{abstract}
    Some explorations not yet ready for the write-up.
\end{abstract}

\maketitle

\section{Maxent formalism}
We consider a distribution $P(\boldsymbol \sigma)$, where $\boldsymbol \sigma$ is an N-dimensional state vector. We search for the distribution which maximizes the entropy subject to some constraints on the expectation value of a (smallish) number of observables:
\begin{equation}
\langle f_\mu(\boldsymbol \sigma)\rangle = \sum_{\boldsymbol \sigma} P(\boldsymbol \sigma) f_\mu(\boldsymbol \sigma) = f_\mu^{emp}
\end{equation}
This leads to Lagrangian of the form,
\begin{equation}
    \mathcal{L}(P(\B \sigma)) = - \sum_{\B \sigma} P(\B \sigma) \ln P(\B \sigma) - \sum_{\mu = 0}^K \lambda_\mu \[[ \langle f_\mu(\boldsymbol \sigma) \rangle - f_\mu^{emp} \]]
\end{equation}
where the index 0 refers to the normalization constraints. Taking the derivative with respect to $P(\B \sigma)$ we derive that the maximum entropy distribution has the following general form:
\begin{equation}
    P(\boldsymbol \sigma) = \frac{1}{Z} \exp\left[ -\sum_{\mu=1}^K \lambda_\mu f_\mu(\boldsymbol \sigma) \right]
\end{equation}


\section{Approach to a lognormal distribution}

See e.g. \url{http://www.scholarpedia.org/article/Cumulants}.

Cumulant of a sum of identically distributed random variables = sum of cumulants. Calculate the cumulants of the amino acid distribution and derive the cumulants of the kmer distribution from there.

One also has $\kappa_n(c X) = c^n \kappa_n(X)$. For $c = 1/N$ this gives the central limit theorem: higher order cumulants of $N^{-1} \sum_i X_i$ vanish more quickly than the first and second order cumulants and the distribution thus converges to a Gaussian distribution which has $\kappa_n = 0$ for $n>2$.

The approach of including more and more of the history in a Markov chain has been used extensively on the nucleotide level: In general an n-th order Markov chain is a model which includes a dependence on the previous n symbols in a sequence.

\section{Mean-field theory}

See the SI of Morcos et al. for a derivation of mean-field results for the Potts model \cite{Morcos2011}. Under the mean-field assumption we have 
\begin{equation}
    J_{ij}^{MF} = - (C^{-1})_{ij}, 
\end{equation}
where $c_{ij} = f_{ij} - f_i f_j$ is the connected correlation function. At small couplings we can expand the inverse of this matrix by noting that its off-diagonal elements are small compared to its diagonal. Setting $C = D + A$ we expand the inverse to first order as
\begin{equation}
    (D+A)^{-1} \approx D^{-1} + D^{-1} A D^{-1}. 
\end{equation}
This yields the even simpler equation
\begin{equation}
    J_{ij}^{MF} = - \rho_{ij},
\end{equation}
where $\rho_{ij} = c_{ij}/(f_i f_j)$ is a normalization of the connected correlation function. This is also the leading order term found by Sessak and Monasson for a small-correlation expansion for the inverse Ising problem.

\end{document}


