\documentclass[superscriptaddress,twocolumn,pre]{revtex4}

\usepackage{ifthen}
\newboolean{pnas}
\setboolean{pnas}{false}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\graphicspath{{images/}}
\usepackage{color}
\usepackage[pdfstartview=FitH,
            breaklinks=true,
            bookmarksopen=false,
            bookmarksnumbered=true,
            colorlinks=true,
            linkcolor=black,
            citecolor=black,
            urlcolor=black,
            pdftitle={Peptidome},
            pdfauthor={Andreas Mayer},
            pdfsubject={}
            ]{hyperref}
\newcommand{\B}{\boldsymbol}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}

\def\(({\left(}
\def\)){\right)}                       
\def\[[{\left[}
\def\]]{\right]}

\newcommand{\AM}[1]{{\color{blue}#1}}

\begin{document}

\title{The statistical ensemble approach to immune discrimination}
%\author{Andreas Mayer}
%\author{Quentin Marcou}
%\author{Warren James}
%\author{Christopher Russo}
%\author{William Bialek*}
%\author{Benjamin D Greenbaum*}
\date{\today}

\begin{abstract}
The immune system needs to distinguish molecular features of pathogens from those found in the organisms' own proteins. A naive but universal way to discriminate, would be to have a system that can recognize any possible foreign peptide, while whitelisting every self peptide that should not elicit a reaction via negative selection. This previously prevailing view has been challenged in recent decades for the T cell system by experiments showing that many self-peptides are not entirely eliminated, but either do not make adequate antigens or are suppressed via regulatory mechanism and that T-cell receptors can be highly non-specific. The implication is that the immune system has adopted a different evolutionary strategy needs to be better understood. To begin to understand this approach we characterize the self and pathogen proteomes as statistical ensembles. Probabilistic models reveal how both universal and phyla-specific constraints on protein evolution shape the statistics of both proteomes. The models furthermore allow us to quantify to what extent the ensembles differ systematically, and hypothesize that the immune system has evolved to maximize the likelihood of recognition based on peptide statistics, since evolution is unlikely to develop strategies for organisms to combat pathogen features they never encounter. We analyze whether and how these differences might be used for an efficient immune defense. Finally, we compare predictions about what would be an efficient immune strategy to what is known about epitopes recognized by the immune system. 
\end{abstract}

\maketitle

\section{Introduction}

A key question in quantitative immunology is how the immune system distinguishes foreign antigens from self-antigens. There is one view of adaptive immunity in which both self and non-self antigens are random samples from a common (and essentially uniform) universe of peptides of a given length. Discrimination is then achieved solely on the basis of "white-listing": thymic negative selection acts to get rid of those T cells that are reactive to self, leaving everything else as a potentially recognizable foreign antigen. If the two types of antigens are instead drawn from different distributions, then some regions of peptide space will be much more likely to be self and some much more likely to be non-self. Over evolutionary timescales the recombination machinery could have evolved to bias the immune repertoire towards recognizing antigens that are more statistically unlikely to arise from the human proteome. Additionally due to different types of proteins and different types of environments, the amino acid and nucleic acid distributions needed for proper functioning of a protein might differ between a host and its pathogen. Alternatively, however, coevolution of pathogens with their hosts might select for pathogens that have a more similar distribution of peptides to their host than they would otherwise have by chance. The ability to address these issues experimentally has drastically changed our view of the T cell recognition machinery over the past decades \cite{Davis2015, Birnbaum2014}. It is clear thymic selection eliminates self reactive T cells only partially. There are many self-reactive T cell in the blood that survive thymic selection, only to be suppressed by peripheral regulation. Moreover, T cell recognition is non-specific, as a cross-reactive T cell maybe be capable of recognizing many hundred peptides. It is therefore clear that self-peptides are eliminated only partially, likely in a biased manner, and that a cross reactive T-cell receptor repertoire may engage in negative selection only partially. 

The question of through what lens the immune system "sees" the world has renewed urgency in the context of cancer immunotherapy \cite{Luksza2017,Balachandran2017,Richman2019}. As it has become clear that the immune system is capable of recognizing altered peptides in a tumor (referred to as neoantigens), which sort of antigens are more easily recognizable by the immune system has become a critical question. If there is a bias present in what makes a good antigen, this would aide in {\it in silico} predictions of response and the development of targets for next generation therapies. Here we revisit the often tacit assumptions about how evolution of the immune machinery might have shaped antigen prediction. In particular, there is a prevailing view of how detecting neoantigens is hard because they are often similar to self. If pathogens are generically very different from self then why would the immune system have evolved the capability of resolving small differences? We question that view by showing that the primary deviations away from a uniform distribution over oligomers are shared between self and non-self peptides. That is, biases exist in both peptide and antigen distributions, but those biases are shared. If it is possible at all to evolve a system to discriminate small differences, then it seems plausible that efficient defense should focus on deviations from "more likely" regions, which the immune system can learn the statistics of from the self-proteome.

%Previous immuno-peptidome analyses have focused on similarity between self and non-self peptides. Work by Claverie and co-workers \cite{Claverie1988} and later by Burroughs, De Boer, and Kesmir \cite{Burroughs2004} demonstrate that the number of shared nonamers decreases with evolutionary distance. The later work discusses some possible slight preference of the antigen processing pathway for non-self antigens. In a follow up work an overlapping set of authors build a tool for immunogenicity prediction based on small differences in amino acid usage in recognized epitopes \cite{Calis2013}, as well as discuss potential large holes created by self-tolerance \cite{Calis2012a}. A more recent follow up by Wortel et al. \cite{Wortel2018} shows that self and foreign peptides are largely similar (much more similar then words from different languages). They argue that under these conditions thymic selection should minimize the co-occurrence of similar self-peptides for efficient self/non-self discrimination based on negative selection. The emerging view would suggest that the immunogenicity of an antigen, via T cell reactivity, is related to how untypical it is given the normal distribution of the human proteome, rather than precision whitelisting. Recent works has found productivity in mutation derived neoantigen immunogenicity prediction both by incorporating distances to immunogenic viral peptides \cite{Luksza}, and distance from the self-proteome \cite{Vonderheide}. In cancer immunology \cite{Walz2015} it has also been known for sometime that immune activation can be achieved not just by neoantigens but also by large changes in protein abundance, such as those arising from epigenetic dysregulation. While mutation derived neoantigens have been implicated in response to checkpoint blockade immunotherapy, they are not the only source of immunogenicity \cite{XXX}. As such, bias towards unlikley peptides would account for both sequence and expression based discrimination.

%Even a single proteome contains a great deal of data. The approximately 20000 genes times 1000 amino acids per gene on average gives $2 \cdot 10^7$ possible {\it k}-mer peptides, when not accounting restrictions on which can be processed and presented \cite{Luksza}. This also implies that most 5mers will still be represented in the proteome, as there are only about $20^5 = 3.2 \cdot 10^6$ possible 5mers. As a consequence, small changes in amino acid usage can lead to larger changes in the relative likelihood of longer stretches of otherwise random sequences: consider a 10\% difference for single amino acids than a random 8mer has a log-likelihood ratio of $1.1^8 \approx 2.15$. Even larger likelihood ratios are expected for long {\it k}-mers once pairwise or higher-order correlations are considered  \cite{Schneidman2006}. In that sense, the ability to capture features of genuine antigens would come from an adequate statistical model that captures typical self-peptides. Karlin and Bucher \cite{Karlin1992} have shown the existence of pairwise correlations in amino acid usage and discuss various structural reasons for these correlations. Peer et al. \cite{Peer2004} have shown that amino acid and oligopeptide compositions differentiate among phyla, which implies one can construct models of self-peptide that are not simply random, and follow up work has shown similar conclusions \cite{Bogatyreva2006}. On the other hand \cite{Lavelle2009} claim that generally 4mers and 5mers do not show large deviations from random models when taking care to remove bias by large protein families, implying the diversification of and selection upon these families account for most organism specific difference. On a more general level the question which structural constraints restrain the evolution of amino acid patterns has received attention for a long time \cite{Turjanski2018}. Random strings of amino acids do not yield valid, folding proteins, but amino acid strings of natural proteins are hard to distinguish from random, other than by deviations from uniform in individual amino acid frequencies that seem to be governed by amino acid mass \cite{BenAlbert}.

%To capture these effects in an ensemble approach, we start by establishing an ensemble model of the self-proteome, demonstrating that most long peptides are random, other than single amino acid biases and a small amount of comparatively weak correlations. We build a maximum entropy distribution constrained to reproduce the amino acid frequencies and the correlations between pairs of amino acids a given distance apart (as we consider random substrings the distribution should only depend on absolute distance). A similar approach was done by Mora et al. \cite{Mora2010} for the distribution of antibodies, however antibodes offer greater antigen specificity than T-cell receptors. In doing so, we construct a theory of immune recognition whereby the species specific biases in the self-peptidome are leveraged to find deviations from the self-distribution by pathogens. As pathogens, specifically viruses, have higher diversity that the self-proteome, yet need to interact with its internal machinery, we argue an immune model based on deviation from the "biased self", rather than trying to target pathogens uniformly and whitelisting, is optimal. 

\section{Results}

\subsection{Discriminability of host and pathogen peptides}


\begin{figure}
    \includegraphics{classifier}
        \caption{{\bf Two views on the discriminability of host and pathogen peptides.} (A) The adaptive immune system interacts with peptides derived from either host or pathogen proteins. How discriminable are peptides drawn from different proteomes? (B) Histograms of loglikelihood ratios for 9mers drawn randomly from a pathogen (Plasmodium falciparum) or human proteome. The likelihood is defined here simply by the average amino acid frequencies in both sets of sequences. (B, inset) Sensitivity (true positive rate - TPR) vs. specificity (false positive rate - FPR) tradeoff curve for a binary classification based on the likelihood ratio. (C) Fraction of pathogen peptides of different length that are distinct from all peptides from the host.
    \label{figclassifier}
    }
\end{figure}

The adaptive immune system of vertebrates discriminates between different peptides, in particular those from the host and pathogen proteomes. Here, using a combination of theoretical and statistical analyses we aim to understand how the biophysics of protein statistics might have shaped the evolution of adaptive immune recognition (Fig.~\ref{figclassifier}). Classification between two sets of sequences can be achieved in a statistical way based on statistical features that differ between the sets of sequences. Such a type of classification underlies all machine learning classification algorithms: Given a training set with items of each class the algorithms aim at generalizing by inferring statistical features that allow discrimination. Instead of learning sequence features discrimination can also be achieved by rote memorization, if the set of sequences is finite and sparsely covers the space of all possible sequences. It is clear that to a certain extent the immune system employs a combination of both strategies. The immune system memorize peptides from the host proteome that should be ignored by a combination of negative selection of specific cells and the induction of regulatory cells. 

To anchor our further explorations we here analyze the ability of both strategies to classify peptides of a given length drawn at random either from a pathogen proteome or the host proteome.

To illustrate when discrimination exploiting finiteness is possible we analyze how the fraction of peptides that are found only in a pathogen protein, but in none of the host proteins depend on the peptide length $k$. Intuitively, the critical value of $k$ beyond, which most pathogen peptides are not to be found in the host proteome, is set by a comparison of the length $L$ of the host proteome to the exponentially increasing number of different kmers $20^k$. Simple combinatorics based on random sequences matches predicts that a random pathogen peptide is not present among the $\sim L$ possible host peptides with a probability $e^{-L/20^k}$ (SI Text~\ref{secsequencematching}). This prediction agrees qualitatively with the empirical data calculated for the same host/pathogen pair considered before (Fig.~\ref{figclassifier}C). Quantitatively, there is a small difference, with the empirical relationship being less steep as a function of $k$ owing to non-randomness in the sequence space coverage. From both theory and data we find that a majority of peptides are distinct for peptides of length $k\geq6$ (orange dots, Fig.~\ref{figclassifier}C), thus setting a minimal length scale needed for this mode of immune discrimination.  


To illustrate statistical discriminability we describe the statistics of each proteome using an independent site model, only taking into account differences in average amino acid usage between proteomes. These models assign a probability $P(\B \sigma)$ to each peptide $\B \sigma$ of the form $P(\B \sigma) = \prod_{i=1}^k P(\sigma_i)$. Given probability functions $P_H(\B \sigma)$ and $P_P(\B \sigma)$ for the host and pathogen, respectively, we can calculate a likelihood ratio as a measure based on which to classify peptides. Fig.~\ref{figclassifier}B show the distributions of likelihood ratios for peptides of a length $k=9$ amino acids from the Plasmodium falciparum proteome (the parasite causing Malaria) with respect to the statistics of human host proteins. By choosing an appropriate cutoff one can turn the likelihood ratio into a binary decision of whether a given peptide originates from one of the two proteomes. We visualize the accuracy with which such a classifier discriminates between host and pathogen peptides as a function of different threshold values in the form of a sensitivity-specificity tradeoff curve (Fig.~\ref{figclassifier}B inset). Here, sensitivity is defined as the fraction of foreign peptides correctly classified as such, and specificity as the fraction of false positives from the host proteome. Quantifying the discrimination accuracy using the area under this curve confirms a substantial statistical discriminability (AUROC = 0.89), although discrimination is not perfect as is clear from the overlap of the distributions of likelihood ratios.



%However, this measure focuses on the overall discriminability, while the immune system might instead make a decision bases on only a few peptides that are particularly easy to discriminate. To assess the accuracy of such a classification strategy, it is particular convenient to display the data in the form of a precision-recall curve (Fig.~\ref{figclassifier}C,F).

A number of questions remain open in our analyses so far, which we will address in turn in the following. First, is there higher order statistical structure in peptide space that might increase discriminability? Second, to the extent that there are statistical differences, do they generalize across pathogens? Third, what are the implications of the statistics for the evolution of immune recognition?


\subsection{A mathematical framework to model peptide statistics} 


\begin{figure*}
    \includegraphics{maxent}
        \caption{{\bf Maximum entropy models of peptide statistics.} A maximum entropy model with third order compositional constraints and second order pairwise constraints on amino acid covariations captures the statistics of the human proteome. (A-C) Comparison of connected correlation functions in the test set with model predictions. (D,E) Density of states relative to the full energy function of models with different types of constraints. (H) Reduction in effective diversity of the peptide distribution resulting from imposing different constraints. The cumulative percentage reduction of effective diversity relative to the first moment model is indicated for each of the nested models.
    \label{figmaxent}
    }
\end{figure*}

We propose to use a maximum entropy framework as a principled way to include increasingly detailed statistical structure into a series nested models of peptide statistics. Using this approach we constrain a set of statistical observables $\langle f_\mu(\boldsymbol \sigma)\rangle$ for $\mu$ from 1 to $K$ to be equal to their empirical values $\bar{f_\mu}$, while otherwise keeping the probability distribution as random as possible. In mathematical terms this means that we are choosing the probability distribution that maximizizes the Shannon entropy
\begin{equation}
    S[P(\B \sigma)] = - \sum_{\B \sigma} P(\B \sigma) \log P(\B \sigma),
\end{equation}
subject to a normalization constraint and constraints that enforce the equality of modelled and empirical marginals
\begin{equation}
    \langle f_\mu(\boldsymbol \sigma)\rangle = \sum_{\boldsymbol \sigma} P(\boldsymbol \sigma) f_\mu(\boldsymbol \sigma) = \bar{f_\mu}
\end{equation}
for each observable.
The optimization with respect to the normalization constraint yields a Boltzmann distribution of the form,
\begin{equation}
    P(\boldsymbol \sigma) = \frac{1}{Z} \exp\left[ -E(\B \sigma) \right].
\end{equation}
Here,
\begin{equation}
 E(\B \sigma) = \sum_{\mu=1}^K \lambda_\mu f_\mu(\boldsymbol \sigma),
\end{equation}
is a sum of terms related to the imposed constraints, which is called the energy in statistical mechanics, and 
\begin{equation}
    Z = \sum_{\B \sigma} \exp \left[ - E(\B \sigma) \right]
\end{equation}
is a normalization factor, called the partition function in statistical mechanics.

We fit the parameters $\lambda_\mu$ to the data using Boltzmann machine learning \cite{Ackley1985}. In short, we estimate $\langle f_\mu(\B \sigma)\rangle$ using Monte Carlo sampling for a given set of model parameters and then update the parameters by iterative scaling (Materials and Methods). To assess potential overfitting we split the dataset into a training and a test set of equal size. We calculate the average of observables used in the fitting on the training set, but compare model predictions against the test set. We can construct a hierarchy of such models including different constraints. We have found that for our purposes a satisfactory model of 9mers drawn from the human proteome is provided by fitting the moments of the amino acid composition of peptides up to third-order. Additionally we fit distance-dependent, but translation invariant 2-point couplings. Taken together these constraints produce a model that recapitulates key statistical features of the data (Fig.~\ref{figmaxent}). In particular, the model reproduces the average frequencies $f_i(\alpha)$ of finding amino acid $\alpha$ at position $i$ (Fig.~\ref{figmaxent}A), as well as the connected two point correlation function $C_{ij}(\alpha, \beta) = f_{ij}(\alpha, \beta) - f_i(\alpha) f_j(\beta)$ (Fig.~\ref{figmaxent}B), and connected three point correlation function $C_{ijk}(\alpha, \beta, \gamma) = f_{ijk}(\alpha, \beta, \gamma) - f_{ij}(\alpha, \beta) f_k(\gamma) - f_{ik}(\alpha, \gamma) f_j(\beta) - f_{jk}(\beta, \gamma) f_i(\alpha) + 2 f_i(\alpha) f_j(\beta) f_k(\gamma)$ (Fig.~\ref{figmaxent}C). The modelled density of states is consistent between the model and the data with a similar number of high probability sequences in the test set and the full model (Fig.~\ref{figmaxent}D,E).
We note that our model does not include any position dependence, as the data is by construction translation-invariant except for edge effects arising from both ends of the protein. 

The importance of higher-order constraints is shown by both the existence of reproducible 2nd and 3rd order connected correlation functions (Fig.~\ref{figmaxent}B,C), as well as an overrepresentation of highly probable sequences relative to predictions of a model only based on the amino acid composition (Fig.~\ref{figmaxent}E,F). To quantify the contribution of different constraints to the reduction of the effective diversity of the ensemble of peptides, we calculate the entropies of the modelled distributions using thermodynamic integration (Material and Methods). We find that the largest reduction of diversity relative to a uniform distribution over all peptides is induced by the biased amino acid usage. The constraints beyond the 1st moment further reduce diversity of the 9mer distribution by 19 percent (Fig.~\ref{figmaxent}G). While the reductions in entropy are modest, the probability with which two randomly drawn kmers coincide is increased by orders of magnitude when including the additional constraints (Fig.~\ref{figmaxent}H)

\subsection{Quantifying differences between host and pathogen proteomes}


\begin{figure}
    \includegraphics{dkls}
    \caption{Kullback-Leibler divergences between peptide distributions drawn from different proteomes relative to human host peptides and relative to a uniform distribution over all peptides. For each proteome we show the statistical distance calculated according to a nested set of models including a different number of constraints. The inset shows a zoom on the set of proteomes close to the human statistics.
    \label{figdkls}
    }
\end{figure}

We compare here a set of proteomes from model organisms across different branches of jawed vertebrates: Human and Mouse as two examples of animals, Chicken as an example of a bird, and Zebrafish as an example of a jawed fish. We also compare them against a set of pathogen proteomes: A collection of human viruses (individual viral proteomes are to small for reliable statistical analyses), a number of bacterial species, as well as parasites (Plasmodium falciparum causing Malaria). For each of these proteomes we fit the same series of maximum entropy models to capture its statistical structure in increasing detail. We then use thermodynamic integration to quantify the statistical distinguishability of each proteome relative to the human proteome in terms of a Kullback-Leibler divergence. The Kullback-Leibler divergence is exactly the expected log-likelihood ratio drawing a peptide from the pathogen distribution versus the human distribution. 

We find that these divergence are small with some notable exceptions (Fig.~\ref{figdkls}). In particular, different vertebrates have very similar statistical structure. Peptides from human viruses are remarkably close in statistical distance to the human proteome. Bacterial peptides differ more substantially, but the divergence is still smaller than 1 nat. However, the bacterial proteomes differ more substantially, in particular Tuberculosis. The largest divergence is determined for the parasite Plasmodium falciparum, which might be a result of the known AT bias of its genome \cite{Hamilton2017}. We also show the divergences with respect to a uniform distribution for reference. We find that natural peptides are generally largely easier to differentiate from random peptides than from peptides from a different proteome. This points towards underlying shared biases in amino acid usage among the different proteomes.

Analyzing the divergences across the model hierarchy we find that constraints beyond amino acid usage only modestly increase the statistical distance with respect to the human host. In most cases the inclusion of additional constraints increases the distance from random peptides more than from human peptides, again pointing at a large degree of conservation of these constraints across proteomes.

\begin{figure*}
    \begin{center}
    \includegraphics{humanviruses}
    \end{center}
    \caption{Performance of the models as classifiers. (A) Distributions of likelihood ratios for peptides from host and virus proteins. (B) Sensitivity-specificity tradeoff curve for various models. (C) Precision-recall characteristics at a 10-fold excess of self-peptides.
    \label{figclassifier}
    }
\end{figure*}

Given the small Kullback-Leibler divergence we expect a modest performance of using these models as classifiers. Indeed, we find that the distribution of likelihood ratios overlap significantly (Fig.~\ref{figclassifier}A), which leads to a modest average classification performance as assessed by the area under the receiver operating curve (Fig.~\ref{figclassifier}B). However, similarly to how the small reduction in entropy went along with a large increase in coincidence probability, discriminability in the tails is also more substantial: for example when focusing discrimination on the top 1\% of most distinguishable viral peptides the precision is roughly four-fold higher using the higher order models than expected by chance, when host peptides are in 10-fold excess. Viral proteome sizes are in the $10^3-10^4$ amino acid range (higher for DNA then RNA viruses). If roughly 10\% of peptides can be presented, the top 1\% corresponds to the top 1 to top 10 peptides.





%Interestingly, the parasite Plasmodium falciparum has the largest observed divergence from the human amino acid usage, . Vaccinia is higher than other viruses, which might attributable to it replicating outside the nucleus of the host cell using its own set of proteins for DNA replication and gene transcription \cite{Tolonen2001}. Mostly the Kullback-Leibler divergences between the doublet distributions are about twice the single site divergences. Notable exceptions are some of the viral proteomes, but this might be due to finite sampling effects biasing the entropy estimates for these small proteomes.

%kUnder the independent model log-likelihoods are additive and thus the average log-likelihood ratio for a kmer is simply k times the calculated single site $D_{KL}$.
%In the limit of large $k$ 
%Chernoff-Stein Lemma!

%\begin{figure}
%    \includegraphics[width=\columnwidth]{viruses}
%    \caption{Likelihood of randomly drawn viral 9mers given a triplet model based on the human proteome statistics. B cell and T cell epitope likelihoods. Interestingly the B cell epitopes are more similar to the human proteome than if they where drawn at random.
%    \label{figviruses}
%    }
%\end{figure}


\subsection{Features of immune epitopes}

\begin{figure}
    \includegraphics[width=\columnwidth]{likelihoodprofile-iedb-tcell}
    \caption{Likelihood of IEDB T cell epitopes given a triplet model based on the human proteome statistics compared to random self peptides. The three distributions are largely indistinguishable except for a slight depletion of epitopes at the highest likelihoods.
    \label{figtcelliedb}
    }
\end{figure}

Which of the possible peptides do eventually get recognized by the immune system? To answer this question we score known immune epitopes from the Immune Epitope Database (IEDB) against a model for the likeliehood of self peptides (Fig.~\ref{figtcelliedb}). There are few epitopes that are very likely under the self statistics, but overall the distributions are largely indistinguishable. There are also no statistical differences between IEDB entries with positive vs. negative assays.

We can zoom into epitopes from particular pathogens to see whether this finding is due to the similarity of the pathogen proteomes or to immune selection.

\section{Discussion}

How does this change our view of the evolution of the adaptive immune system?

Cancer neoantigens might not be that special! Due to underlying shared biases many more antigens from 
pathogens are close to self than otherwise expected.



\section{Material and Methods}

\subsection{Model construction and fitting}

A common choice is to constrain the one and two-point frequencies
\begin{align}
    f_i^\alpha(\B \sigma) &= \sigma_i^\alpha, \\
    f_{ij}^{\alpha\beta}(\B \sigma) &= \sigma_i^\alpha \sigma_j^\beta,
\end{align}
where $\sigma_i^\alpha = 1$ if the amino acid at site i is of type $\alpha$ and zero otherwise.
This leads to a maximum entropy probability distribution that takes the form of a disordered Potts model,
\begin{equation}
    E(\boldsymbol \sigma) = - \sum_{i=1}^L \sum_{\alpha = 1}^{20} h_i^\alpha \sigma_i^\alpha - \sum_{i<j}^L \sum_{\alpha,\beta = 1}^{20} J_{ij}^{\alpha \beta}  \sigma_i^\alpha \sigma_j^\beta.
\end{equation}

Given that many biases are compositional in nature we can also consider a simpler model that only involves a global constraint on the covariation of the total count of amino acids of different types:
\begin{align}
    n^\alpha(\B \sigma) &= \sum_i \sigma_i^\alpha, \\
    n^{\alpha\beta}(\B \sigma) &= n^\alpha(\B \sigma) n^\beta(\B\sigma) = \left(\sum_{i=1}^L \sigma_i^\alpha\right) \left(\sum_{j=1}^L \sigma_j^\beta\right).
\end{align}
This leads to a maximum entropy probability distribution that takes the form
\begin{equation}
    E(\boldsymbol \sigma) = - \sum_{\alpha=1}^{20} h^\alpha n^\alpha -  \sum_{\alpha,\beta = 1}^{20} J^{\alpha \beta} n^\alpha n^\beta,
\end{equation}
i.e. a model that only involves global couplings between amino acids independent of their distance.


The parameters are updated by iterative scaling,
\begin{equation}
    \lambda_\mu^{t+1} = \lambda_\mu^t + \epsilon_\mu^t \log \left( \frac{\langle f_\mu \rangle}{\bar{f_\mu}} \right),
\end{equation}
where $\epsilon_\mu^t$ represents a learning rate (which generally can be coordinate dependent and time-varying).
%The parameters are updated by gradient ascent,
%\begin{equation}
%    \lambda_\mu^{t+1} = \lambda_\mu^t + \epsilon_\mu^t \left(\langle f_\mu \rangle  - \bar{f_\mu}\right),
%\end{equation}
%where $\epsilon_\mu^t$ represents a learning rate (which generally can be coordinate dependent and time-varying).

Once we have fitted the model parameters we can calculate the entropy of the distribution $P(\B \sigma)$. To do so we use the identity
\begin{align}
    S &= - \sum_{\B \sigma}  P(\B \sigma) \log P(\B \sigma),  \\
      &= \langle E(\B \sigma) \rangle - F, \quad F = - \log Z.
\end{align}
The mean energy can be calculated directly from Monte Carlo samples. To calculate the free energy we use thermodynamic integration as follows \cite{Marchi2019b}: We express the energy with respect to a reference energy $E_{ref}(\B \sigma)$ as $E(\B \sigma) = E_{ref}(\B \sigma) + \Delta E(\B\sigma)$, where the reference energy is choosen such that $F_{ref}$ can be calculated analytically. We define the perturbed energy function
\begin{equation}
    E_\alpha(\B \sigma) = E_{ref}(\B \sigma) + \alpha \Delta E(\B\sigma),
\end{equation}
which scales the contribution of the energy beyond the reference model.
To obtain the free energy we use the identity
\begin{equation}
    F(1) = F_{ref} + \int_0^1 \ud \alpha F'(\alpha).
\end{equation}
Note that
\begin{equation}
    F'(\alpha) = - \langle \Delta E(\B \sigma) \rangle_{\alpha},
\end{equation}
which allows us to approximate the integrand by Monte Carlo simulations. We numerically evaluate $F'(\alpha)$ for evenly spaced $\alpha \in [0, 1]$ and calculate the integral by Simpson's rule. Calculating the entropy in this way allows us to determine the reduction in the effective diversity in sequences implied by including different constraints (Fig.~\ref{figmaxent}F). 

\bibliographystyle{apsrev}
\bibliography{library}



\section{A shell theory of immunogenicity?}

\begin{figure}
    \includegraphics{shelltheorysketch}
    \caption{Sketch of the shell theory of immunogenicity: To be immunogenic an antigen needs to fall outside the tolerance region (orange), but within one of the detectable regions surrounding a self-antigen (blue).
    \label{figshelltheorysketch}
    }
\end{figure}

\begin{figure}
    \includegraphics[width=\columnwidth]{shelltheory}
    \caption{Immunogenicity as a function of the likelihood a peptide in the shell model. The probability of a peptide being immunogenic is defined as the probability of being detectable, but not tolerated. The probability of immunogenicity times the probability of peptides within the pathogen proteome (assumed to be lognormal with $mu = -1.25 k$ and $\sigma = 0.3 k$ with $k=9$) gives a hypothetical expected distribution for the epitopes, which is clipped at both ends.  
    \label{figshelltheory}
    }
\end{figure}

How likely is it that a peptide will be close enough to a self-peptide to be tolerized? How likely to be close enough such that there has been positive selection? To a first approximation neighboring peptides will have similar probabilities. Therefore the probabilities of a peptide relate to the local density and thus to the average distance to the nearest neighbor.

Consider that there is a number $N_T$ neighboring peptides that if they are in the self-peptidome would lead to tolerance, and a number $N_D > N_T$ of neighboring peptides that would lead to positive selection on TCRs recognizing the peptide and are thus needed for detectability. The probability of a peptide of probability $p$ being tolerized is $P_T \sim 1-(1-N_T p)^N$ assuming the $N$ possible self-peptides are sampled statistically independently from the peptidome distribution. As $N_T p \ll 1$ we can approximate $P_T \sim 1-e^{-N_T p N}$. Similarly, the probability of being detectable is $P_D \sim 1-(1-N_D p)^N \approx 1-e^{-N_D p N}$. Setting $N_T$ to the number of first neighbors (171 for 9mers), and $N_D$ to the number of second neighbors (25992 for 9mers) and assuming that $N$ is equal to the total possible number of kmers that can be generated from the full human proteome ($\sim 2 \cdot 10^7$), we can plot how $P_T$ and $P_D$ depends on $p$ (Fig.~\ref{figshelltheory}).



\begin{figure}
    \includegraphics{entropyaa}
    \caption{Entropy of the single site distribution of amino acids used in different positions along proteins in the human proteome. In an overwhelming majority of cases the first amino acid in a protein is a Methionine, because this amino acid is encoded by the start codon AUG. There is a slightly lower entropy o amino acids also in the following few amino acids, which might similarly be explained by known mRNA biases around the initiation codon (rf. Kozak consensus sequence). 
    \label{figentropyaa}
    }
\end{figure}


\section{Probability of random sequence matches}
\label{secsequencematching}
Given a random set of $L$ sequences of length $k$ what is the probability with which a new randomly chosen sequence is going to match any of the existing sequences?
There are $20^k$ sequences of length $k$ and all of them we assume here are sampled with equal probability. Thus for the new sequenc will match any of the existing sequences with a probability $p_0 = 1/20^k$. Matches to different sequences can be treated as independent of each other, such that the probability of not matching any of the sequences is given by
\begin{equation}
    P(\mathrm{no match}) = (1-p_0)^L \sim e^{-L / 20^k},
\end{equation}
where in the last step we have used that $20^k \ll 1$ for $k \ll 1$.


\input{independentmodel}

\subsection{Proteins are surprisingly random}

\begin{figure}
    \includegraphics{entropykmer}
    \caption{Entropy of the kmer distribution of the human and yeast proteome. The solid black line shows the maximal entropy of the flat distribution over the 20 possible amino acids $\log_2 20$. Note how the biggest reduction in entropy comes from amino acid biases with only a small further reduction by correlations in amino acid usage.
    \label{figentropykmer}
    }
\end{figure}


\begin{figure}
    \includegraphics[width=\columnwidth]{mutualinformationdecay}
    \caption{Mutual information between amino acids a certain distance apart in the human and mouse proteome. The mutual information is small as compared to the entropy of the single site $\sim 4$ bits, but extends to long distances. The prominent peaks at a distance that is a multiple of 28 amino acids in the human proteome are likely due to Zinc-finger repeats, more particularly C2H2 fingers (cf. high fold enrichments of cysteine and histdine at a distance of 28) \cite{Krishna2003}. 
    \label{figmutualinformationdecay}
    }
\end{figure}

How close to random is the protein distribution? To answer this question we calculate how much information about the amino acid in a certain position is revealed by knowing its neighbors. Concretely, we calculate the mutual information between amino acid pair a certain distance apart. The mutual information can be expressed as $I(X, Y) = H(X) + H(Y) - H(X, Y)$. We use this expression to calulate the mutual information from the single site entropies and the entropy of the joint distribution, which we calculate using the estimator proposed by Grassberger \cite{Grassberger2003}. Note that one cannot assume uniform single site entropies across all comparions stemming most notably from differences in amino acid composition for the first few amino acids within a protein (Fig.~\ref{figentropyaa}). The universal constraints on unrelated proteins are surprisingly small as measured by the mutual information (Fig.~\ref{figmutualinformationdecay}, see also \cite{Lavelle2009}). 

\begin{figure}
    \includegraphics[width=\columnwidth]{4mer-comparison}
    \caption{Performance of the Maxent model for predicting the 4mer frequencies. Using an even split of the data into a test and a training set we compare predictions on the test set with the reproducibility of the frequencies between the training and test set.
    \label{fig4mermaxent}
    }
\end{figure}

\begin{table}
    \begin{center}
        \begin{tabular}{ c c }
            distribution & 4mer DKL\\ \hline
            uniform & 0.1115\\
            independent & 0.0195\\
            1st order Markov chain & 0.0132\\
            Pairwise Maxent & 0.0092\\
            2nd order Markov chain & 0.0087\\
            test & 0.0076\\
        \end{tabular}
    \end{center}
    \caption{Comparison of models based on 4mer frequencies}
\end{table}

We can further dissect which correlations between individual amino acids contribute to the mutual information. To do so we analyze the fold enrichment of amino acid doublets relative to an independent model. Most doublets are only over- or underrepresented moderately (Fig.~\ref{figdoubletentrichment}). An exception is a strong enrichment of Cysteine-Cysteine 3 amino acids apart. This enrichment has been found previously \cite{Greenbaum2014} and is likely due to a particular preference of cysteines to form disulfide bonds at this distance. Another prominent feature of this data are the relatively high enrichments along the diagonal at all distances. This might be attributable to amino acid repeats within some proteins \cite{Turjanski2018}.

\begin{figure}
    \includegraphics[width=\columnwidth]{doubletenrichment}
    \caption{Doublets are only enriched moderately relative to independent amino acid choice. (The enrichments are statistically significant, however. There are $\sim 10^7$ amino acids in the human proteome and thus on the order of $\sim 10^4$ counts per doublet. The relative error on the frequencies is thus on the order of only $\sim 1/\sqrt{10^4} = 10^{-2}$.)
    \label{figdoubletentrichment}
    }
\end{figure}

\subsection{The distribution of kmer likelihoods is close to being lognormal}

\begin{figure}
    \includegraphics{lognormalaa}
    \caption{Histogram of amino acid frequencies in the human proteome and lognormal approximation. (A) Probability density of the empirical likelihoods of a randomly drawn single amino acid and lognormal approximation. (B) Probability density of the empirical likelihoods of a randomly drawn 4-mer and lognormal prediction based on an independent model.
    (C) Probability density of the likelihoods of randomly drawn 9-mers under an independent model and lognormal prediction.
    \label{figlognormalaa}
    }
\end{figure}

As the two-point correlations are small we can try and approximate the likelihood of a given kmer in a proteome by an independent site model,
\begin{equation}
    P(s_1s_2 ... s_k) = \prod_{i=1}^k P(s_i),
\end{equation}
or more conveniently, their log-likelihood
\begin{equation}
    \ln P(s_1s_2 ... s_k) = \sum_{i=1}^k \ln P(s_i),
\end{equation}
which is a sum of independent random variables under the independent site model.
Using the central limit theorem we can approximate the distribution of the log-likelihoods of a kmer by a normal distribution. To do so we need to know the mean $\mu$ and variance $\sigma^2$ of the likelihood of a randomly picked amino acid. These can be calculated directly from the probability mass function of amino acid frequencies (Fig.~\ref{figlognormalaa}A). Note that the distribution of amino acid frequencies is somewhat skewed (skewness $\gamma_1 = -1.05$), but this skewness should diminish with $k$. The kmer distribution is approximately normal with mean $k \mu$ and variance $k \sigma^2$ as the cumulants of the sum of independent variables is equal to the sum of the cumulants.  Indeed, using the mean and variance of the distribution of frequencies of randomly chosen amino acids (Fig.~\ref{figlognormalaa}A), predicts relatively well the mean and variance of the empirical likelihoods of 4mers (Fig.~\ref{figlognormalaa}B) as well as of the model-based likelihoods of 9mers (Fig.~\ref{figlognormalaa}C). There are some deviations from the lognormal approximation apparent in both cases. There is a slight shift towards higher mean likelihoods, corresponding to a slightly reduced entropy in line with our earlier findings. Additionally, there are enrichments at the lower tail in both distributions, and the 4mer data shows an additional enrichment at the high likelihood tail.




\begin{table}
    \begin{center}
        \begin{tabular}{ c c c }
            Organism&site $D_{KL}$&doublet $D_{KL}$\\
            \hline
Mouse&0.00023&0.00089\\
Yeast&0.046&0.097\\
Cockroach&0.015&0.033\\
Cattle&0.00031&0.0011\\
Chicken&0.0011&0.0038\\
Wheat&0.012&0.029\\
Soybean&0.013&0.033\\\hline
Vaccinia&0.13&0.27\\
InfluenzaB&0.043&0.17\\
InfluenzaA&0.042&0.17\\
CMV&0.035&0.085\\
HCV&0.064&0.2\\
HBV&0.078&0.28\\
DENV&0.049&0.19\\
HIV&0.043&0.19\\
EBV&0.043&0.1\\ \hline
Human viruses&0.015&0.038\\\hline
Ebola&0.023&0.12\\
Tuberculosis&0.11&0.23\\
Listeria&0.071&0.15\\
Burkholderia&0.1&0.23\\
Meningococcus&0.039&0.1\\
StreptococcusPyogenes&0.06&0.14\\
Hpylori&0.077&0.19\\
Lyme&0.19&0.38\\
Tetanus&0.15&0.3\\ 
Leprosy&0.077&0.17\\ \hline
Malaria&0.36&0.71\\
Chagas&0.017&0.051\\
OnchocercaVolvulus&0.03&0.069\\
        \end{tabular}
    \end{center}
    \caption{Kullback-Leibler divergence between amino acid frequencies (and doublet frequencies) in various proteomes and the human proteome (in bits). Human viruses: all viruses with human host from uniprot filtered for duplicates using the Uniref 90 criteria.}
    \label{tabkldiv}
\end{table}



\end{document}
